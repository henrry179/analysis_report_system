# Claudeç³»åˆ—æ¨¡å‹å®ç°ç¤ºä¾‹ / Claude Series Model Implementation Examples

> æœ¬æ–‡æ¡£æä¾›Anthropic Claudeç³»åˆ—æ¨¡å‹çš„å®Œæ•´å®ç°ç¤ºä¾‹ã€æœ€ä½³å®è·µå’Œå®é™…åº”ç”¨æ¡ˆä¾‹
> This document provides comprehensive implementation examples, best practices, and real-world application cases for Anthropic's Claude series models

**æœ€åæ›´æ–° / Last updated: 2025å¹´09æœˆ02æ—¥ 11:35:28**

---

## ğŸ“‹ ç›®å½• / Table of Contents

- [æ¦‚è¿° / Overview](#æ¦‚è¿°--overview)
- [ç¯å¢ƒé…ç½® / Environment Setup](#ç¯å¢ƒé…ç½®--environment-setup)
- [åŸºç¡€è°ƒç”¨ç¤ºä¾‹ / Basic Calling Examples](#åŸºç¡€è°ƒç”¨ç¤ºä¾‹--basic-calling-examples)
- [é«˜çº§åŠŸèƒ½ç¤ºä¾‹ / Advanced Feature Examples](#é«˜çº§åŠŸèƒ½ç¤ºä¾‹--advanced-feature-examples)
- [å·¥å…·è°ƒç”¨é›†æˆ / Tool Calling Integration](#å·¥å…·è°ƒç”¨é›†æˆ--tool-calling-integration)
- [æµå¼å“åº”å¤„ç† / Streaming Response Handling](#æµå¼å“åº”å¤„ç†--streaming-response-handling)
- [é”™è¯¯å¤„ç†ä¸é‡è¯• / Error Handling and Retry](#é”™è¯¯å¤„ç†ä¸é‡è¯•--error-handling-and-retry)
- [æ€§èƒ½ä¼˜åŒ– / Performance Optimization](#æ€§èƒ½ä¼˜åŒ–--performance-optimization)
- [å®é™…åº”ç”¨æ¡ˆä¾‹ / Real-world Application Cases](#å®é™…åº”ç”¨æ¡ˆä¾‹--real-world-application-cases)
- [æœ€ä½³å®è·µ / Best Practices](#æœ€ä½³å®è·µ--best-practices)

---

## æ¦‚è¿° / Overview

### Claudeæ¨¡å‹ç³»åˆ— / Claude Model Series

Anthropicçš„Claudeç³»åˆ—æ¨¡å‹ä»¥å…¶å“è¶Šçš„å®‰å…¨æ€§ã€å¯é æ€§ä»¥åŠåœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶çš„å‡ºè‰²è¡¨ç°è€Œé—»åã€‚

**ä¸»è¦æ¨¡å‹ç‰ˆæœ¬ / Main Model Versions:**
- **Claude 3.5 Sonnet**: æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
- **Claude 3 Opus**: åŠŸèƒ½æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œé€‚åˆé«˜åº¦å¤æ‚çš„ä»»åŠ¡
- **Claude 3 Haiku**: é€Ÿåº¦æœ€å¿«çš„æ¨¡å‹ï¼Œé€‚åˆç®€å•ä»»åŠ¡
- **Claude 3 Sonnet**: å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦çš„é€šç”¨æ¨¡å‹

### æ ¸å¿ƒä¼˜åŠ¿ / Core Advantages

```xml
<claude_advantages>
  <safety_focused>
    <!-- å®‰å…¨å¯¼å‘ / Safety-focused -->
    <constitutional_ai>å®ªæ”¿AIè®¾è®¡ / Constitutional AI design</constitutional_ai>
    <jailbreak_resistant>æŠ—è¶Šç‹±æ”»å‡» / Jailbreak resistant</jailbreak_resistant>
    <content_filtering>æ™ºèƒ½å†…å®¹è¿‡æ»¤ / Intelligent content filtering</content_filtering>
  </safety_focused>

  <capability_strengths>
    <!-- èƒ½åŠ›ä¼˜åŠ¿ / Capability strengths -->
    <long_context>é•¿ä¸Šä¸‹æ–‡ç†è§£ / Long context understanding</long_context>
    <reasoning_ability>å¼ºå¤§çš„æ¨ç†èƒ½åŠ› / Strong reasoning ability</reasoning_ability>
    <tool_integration>ä¼˜ç§€çš„å·¥å…·é›†æˆ / Excellent tool integration</tool_integration>
  </capability_strengths>

  <reliability_features>
    <!-- å¯é æ€§ç‰¹æ€§ / Reliability features -->
    <consistent_responses>ä¸€è‡´æ€§å“åº” / Consistent responses</consistent_responses>
    <predictable_behavior>å¯é¢„æµ‹è¡Œä¸º / Predictable behavior</predictable_behavior>
    <error_handling>ä¼˜é›…é”™è¯¯å¤„ç† / Graceful error handling</error_handling>
  </reliability_features>
</claude_advantages>
```

---

## ç¯å¢ƒé…ç½® / Environment Setup

### ä¾èµ–å®‰è£… / Dependencies Installation

```bash
# Pythonç¯å¢ƒ / Python Environment
pip install anthropic
pip install python-dotenv  # ç”¨äºç¯å¢ƒå˜é‡ç®¡ç† / For environment variable management

# æˆ–è€…ä½¿ç”¨Conda / Or use Conda
conda install -c conda-forge anthropic
```

### APIå¯†é’¥é…ç½® / API Key Configuration

```python
# .envæ–‡ä»¶é…ç½® / .env file configuration
# ANTHROPIC_API_KEY=your_api_key_here

# Pythonä»£ç åŠ è½½ / Python code loading
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv('ANTHROPIC_API_KEY')
```

### å®¢æˆ·ç«¯åˆå§‹åŒ– / Client Initialization

```python
import anthropic

# åŸºç¡€å®¢æˆ·ç«¯åˆå§‹åŒ– / Basic client initialization
client = anthropic.Anthropic(
    api_key=api_key,
)

# å¸¦ä»£ç†çš„å®¢æˆ·ç«¯åˆå§‹åŒ– / Client initialization with proxy
client = anthropic.Anthropic(
    api_key=api_key,
    proxies={
        'http': 'http://proxy.example.com:8080',
        'https': 'http://proxy.example.com:8080'
    }
)

# è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯ / Custom HTTP client
import httpx

http_client = httpx.Client(
    proxies={'http://': 'http://proxy.example.com:8080'},
    timeout=60.0
)

client = anthropic.Anthropic(
    api_key=api_key,
    http_client=http_client
)
```

---

## åŸºç¡€è°ƒç”¨ç¤ºä¾‹ / Basic Calling Examples

### ç®€å•æ–‡æœ¬ç”Ÿæˆ / Simple Text Generation

```python
# åŸºç¡€æ–‡æœ¬ç”Ÿæˆ / Basic text generation
def simple_text_generation():
    """ClaudeåŸºç¡€æ–‡æœ¬ç”ŸæˆåŠŸèƒ½ / Basic Claude text generation"""

    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1000,
        temperature=0.7,
        system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
        messages=[
            {
                "role": "user",
                "content": "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
            }
        ]
    )

    return message.content[0].text

# è°ƒç”¨ç¤ºä¾‹ / Usage example
response = simple_text_generation()
print(response)
```

### å¤šè½®å¯¹è¯ / Multi-turn Conversation

```python
def multi_turn_conversation():
    """å¤šè½®å¯¹è¯å®ç° / Multi-turn conversation implementation"""

    conversation_history = []

    def add_message(role, content):
        """æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å² / Add message to conversation history"""
        conversation_history.append({
            "role": role,
            "content": content
        })

    def get_response():
        """è·å–Claudeçš„å›å¤ / Get Claude's response"""
        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0.7,
            system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ã€‚",
            messages=conversation_history
        )
        return message.content[0].text

    # å¯¹è¯ç¤ºä¾‹ / Conversation example
    add_message("user", "ä½ å¥½ï¼Œæˆ‘æ­£åœ¨å­¦ä¹ Pythonç¼–ç¨‹ã€‚")
    response1 = get_response()
    print(f"Claude: {response1}")

    add_message("assistant", response1)
    add_message("user", "èƒ½å¸®æˆ‘è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼å—ï¼Ÿ")
    response2 = get_response()
    print(f"Claude: {response2}")

    return conversation_history

# ä½¿ç”¨ç¤ºä¾‹ / Usage example
conversation = multi_turn_conversation()
```

### è‡ªå®šä¹‰ç³»ç»Ÿæç¤º / Custom System Prompt

```python
def custom_system_prompt():
    """è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ / Custom system prompt"""

    system_prompts = {
        "code_reviewer": """
        ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚
        è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¯„ä¼°ä»£ç ï¼š
        1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
        2. æ€§èƒ½å’Œæ•ˆç‡
        3. å®‰å…¨æ€§å’Œæœ€ä½³å®è·µ
        4. å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§

        è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
        """,

        "technical_writer": """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£æ’°å†™è€…ã€‚
        è¯·ç¡®ä¿æ–‡æ¡£ï¼š
        1. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘è¿è´¯
        2. æ¦‚å¿µå‡†ç¡®ï¼Œç”¨è¯ç²¾ç¡®
        3. ç¤ºä¾‹ä¸°å¯Œï¼Œæ˜“äºç†è§£
        4. æ ¼å¼è§„èŒƒï¼Œä¾¿äºé˜…è¯»

        ä½¿ç”¨Markdownæ ¼å¼è¾“å‡ºã€‚
        """,

        "business_analyst": """
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±å•†ä¸šåˆ†æå¸ˆã€‚
        åœ¨åˆ†æé—®é¢˜æ—¶ï¼Œè¯·ï¼š
        1. è¯†åˆ«æ ¸å¿ƒä¸šåŠ¡éœ€æ±‚
        2. åˆ†æå¸‚åœºæœºä¼šå’Œé£é™©
        3. æå‡ºå¯è¡Œæ€§è§£å†³æ–¹æ¡ˆ
        4. è¯„ä¼°æŠ•èµ„å›æŠ¥ç‡

        è¯·ç”¨æ•°æ®å’Œäº‹å®æ”¯æ’‘ä½ çš„åˆ†æã€‚
        """
    }

    def analyze_with_role(role, content):
        """ä½¿ç”¨æŒ‡å®šè§’è‰²åˆ†æå†…å®¹ / Analyze content with specified role"""

        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¾“å‡º / Lower temperature for more stable output
            system=system_prompts[role],
            messages=[
                {
                    "role": "user",
                    "content": content
                }
            ]
        )

        return message.content[0].text

    # ä½¿ç”¨ç¤ºä¾‹ / Usage examples
    code_sample = """
    def calculate_fibonacci(n):
        if n <= 1:
            return n
        return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)
    """

    review_result = analyze_with_role("code_reviewer", f"è¯·å®¡æŸ¥è¿™æ®µä»£ç ï¼š\n{code_sample}")
    print("ä»£ç å®¡æŸ¥ç»“æœ:")
    print(review_result)

    return analyze_with_role

# æµ‹è¯•ä¸åŒè§’è‰² / Test different roles
analyzer = custom_system_prompt()
```

---

## é«˜çº§åŠŸèƒ½ç¤ºä¾‹ / Advanced Feature Examples

### å¤æ‚æ¨ç†ä»»åŠ¡ / Complex Reasoning Tasks

```python
def complex_reasoning_task():
    """å¤æ‚æ¨ç†ä»»åŠ¡ç¤ºä¾‹ / Complex reasoning task example"""

    def step_by_step_analysis(problem):
        """é€æ­¥åˆ†æé—®é¢˜ / Step-by-step problem analysis"""

        prompt = f"""
        è¯·é€æ­¥åˆ†æå¹¶è§£å†³ä»¥ä¸‹é—®é¢˜ã€‚æ¯ä¸ªæ­¥éª¤éƒ½è¦è¯¦ç»†è¯´æ˜ä½ çš„æ¨ç†è¿‡ç¨‹ï¼š

        é—®é¢˜ï¼š{problem}

        è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š
        1. ç†è§£é—®é¢˜ï¼šæ˜ç¡®é—®é¢˜çš„æ ¸å¿ƒè¦ç´ å’Œçº¦æŸæ¡ä»¶
        2. æ”¶é›†ä¿¡æ¯ï¼šè¯†åˆ«éœ€è¦çš„å…³é”®ä¿¡æ¯å’Œæ•°æ®
        3. åˆ¶å®šæ–¹æ¡ˆï¼šæå‡ºå¯èƒ½çš„è§£å†³æ–¹æ¡ˆå¹¶è¯„ä¼°ä¼˜ç¼ºç‚¹
        4. åšå‡ºå†³ç­–ï¼šåŸºäºåˆ†æç»“æœé€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
        5. å®æ–½è®¡åˆ’ï¼šåˆ¶å®šå…·ä½“çš„æ‰§è¡Œæ­¥éª¤å’Œæ—¶é—´å®‰æ’
        6. é£é™©è¯„ä¼°ï¼šè¯†åˆ«æ½œåœ¨çš„é£é™©å’Œåº”å¯¹ç­–ç•¥

        è¯·ç”¨ä¸­æ–‡è¯¦ç»†å›ç­”æ¯ä¸ªæ­¥éª¤ã€‚
        """

        message = client.messages.create(
            model="claude-3-opus-20240229",  # ä½¿ç”¨æœ€å¼ºå¤§çš„æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç† / Use the most powerful model for complex reasoning
            max_tokens=4000,
            temperature=0.1,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„æ¨ç† / Lower temperature for more accurate reasoning
            system="ä½ æ˜¯ä¸€ä¸ªé€»è¾‘ä¸¥è°¨çš„åˆ†æå¸ˆï¼Œæ“…é•¿ç³»ç»Ÿæ€§é—®é¢˜è§£å†³ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        return message.content[0].text

    # ç¤ºä¾‹é—®é¢˜ / Example problem
    business_problem = """
    æˆ‘ä»¬çš„ç”µå•†å¹³å°æœ€è¿‘ç”¨æˆ·æµå¤±ç‡ä¸Šå‡äº†15%ã€‚
    ä¸»è¦ç°è±¡ï¼š
    - è´­ç‰©è½¦æ”¾å¼ƒç‡ä»25%ä¸Šå‡åˆ°35%
    - ç”¨æˆ·å¤è´­ç‡ä¸‹é™10%
    - å®¢æˆ·æœåŠ¡æŠ•è¯‰å¢åŠ 20%

    è¯·åˆ†æå¯èƒ½çš„åŸå› å¹¶æå‡ºè§£å†³æ–¹æ¡ˆã€‚
    """

    analysis_result = step_by_step_analysis(business_problem)
    print("å¤æ‚é—®é¢˜åˆ†æç»“æœ:")
    print(analysis_result)

    return analysis_result

# æ‰§è¡Œå¤æ‚æ¨ç†ä»»åŠ¡ / Execute complex reasoning task
result = complex_reasoning_task()
```

### åˆ›æ„å†…å®¹ç”Ÿæˆ / Creative Content Generation

```python
def creative_content_generation():
    """åˆ›æ„å†…å®¹ç”Ÿæˆç¤ºä¾‹ / Creative content generation example"""

    def generate_creative_content(content_type, topic, constraints=None):
        """ç”Ÿæˆåˆ›æ„å†…å®¹ / Generate creative content"""

        constraint_text = ""
        if constraints:
            constraint_text = f"\n\nçº¦æŸæ¡ä»¶ï¼š\n" + "\n".join(f"- {c}" for c in constraints)

        prompt = f"""
        è¯·ä½œä¸ºä¸€ååˆ›æ„ä¸“å®¶ï¼Œç”Ÿæˆ{topic}ç›¸å…³çš„{content_type}ã€‚

        è¦æ±‚ï¼š
        1. åˆ›æ„ç‹¬ç‰¹ï¼Œæœ‰æ–°æ„
        2. å†…å®¹å®Œæ•´ï¼Œç»“æ„æ¸…æ™°
        3. å…·æœ‰å®ç”¨ä»·å€¼
        4. æ˜“äºç†è§£å’Œåº”ç”¨{constraint_text}

        è¯·æä¾›å®Œæ•´çš„{content_type}å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€ä¸»è¦å†…å®¹å’Œæ€»ç»“ã€‚
        """

        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=3000,
            temperature=0.8,  # æé«˜æ¸©åº¦ä»¥å¢åŠ åˆ›æ„æ€§ / Increase temperature for more creativity
            system="ä½ æ˜¯ä¸€ä¸ªå¯Œæœ‰åˆ›æ„çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆåˆ›æ–°æ€§çš„å†…å®¹ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )

        return message.content[0].text

    # åˆ›æ„å†…å®¹ç”Ÿæˆç¤ºä¾‹ / Creative content generation examples

    # ç¤ºä¾‹1ï¼šç”Ÿæˆäº§å“ä»‹ç»æ–‡æ¡ˆ / Example 1: Generate product introduction copy
    product_copy = generate_creative_content(
        content_type="äº§å“ä»‹ç»æ–‡æ¡ˆ",
        topic="æ™ºèƒ½å®¶å±…æ§åˆ¶ç³»ç»Ÿ",
        constraints=[
            "é¢å‘å¹´è½»å®¶åº­ç”¨æˆ·",
            "çªå‡ºæ˜“ç”¨æ€§å’Œæ™ºèƒ½åŒ–",
            "åŒ…å«æ ¸å¿ƒåŠŸèƒ½å’ŒæŠ€æœ¯ä¼˜åŠ¿",
            "å­—æ•°æ§åˆ¶åœ¨300å­—ä»¥å†…"
        ]
    )
    print("äº§å“ä»‹ç»æ–‡æ¡ˆ:")
    print(product_copy)
    print("\n" + "="*50 + "\n")

    # ç¤ºä¾‹2ï¼šç”Ÿæˆæ•™è‚²å†…å®¹ / Example 2: Generate educational content
    tutorial_content = generate_creative_content(
        content_type="ç¼–ç¨‹æ•™ç¨‹",
        topic="Pythonå¼‚æ­¥ç¼–ç¨‹",
        constraints=[
            "é€‚åˆåˆå­¦è€…",
            "åŒ…å«å®é™…ä»£ç ç¤ºä¾‹",
            "è§£é‡Šæ ¸å¿ƒæ¦‚å¿µ",
            "æä¾›ç»ƒä¹ å»ºè®®"
        ]
    )
    print("ç¼–ç¨‹æ•™ç¨‹:")
    print(tutorial_content)

    return generate_creative_content

# æ‰§è¡Œåˆ›æ„å†…å®¹ç”Ÿæˆ / Execute creative content generation
creative_generator = creative_content_generation()
```

---

## å·¥å…·è°ƒç”¨é›†æˆ / Tool Calling Integration

### åŸºç¡€å·¥å…·è°ƒç”¨ / Basic Tool Calling

```python
def basic_tool_calling():
    """åŸºç¡€å·¥å…·è°ƒç”¨ç¤ºä¾‹ / Basic tool calling example"""

    # å®šä¹‰å·¥å…· / Define tools
    tools = [
        {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "input_schema": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°"
                    }
                },
                "required": ["city"]
            }
        },
        {
            "name": "calculate",
            "description": "æ‰§è¡Œæ•°å­¦è®¡ç®—",
            "input_schema": {
                "type": "object",
                "properties": {
                    "expression": {
                        "type": "string",
                        "description": "æ•°å­¦è¡¨è¾¾å¼"
                    }
                },
                "required": ["expression"]
            }
        }
    ]

    def get_weather(city):
        """æ¨¡æ‹Ÿè·å–å¤©æ°”ä¿¡æ¯ / Simulate getting weather information"""
        # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„å¤©æ°”APIè°ƒç”¨ / This should be actual weather API call
        weather_data = {
            "åŒ—äº¬": {"temperature": 25, "condition": "æ™´æœ—", "humidity": 60},
            "ä¸Šæµ·": {"temperature": 28, "condition": "å¤šäº‘", "humidity": 75},
            "æ·±åœ³": {"temperature": 30, "condition": "ç‚çƒ­", "humidity": 80}
        }
        return weather_data.get(city, {"temperature": 20, "condition": "æœªçŸ¥", "humidity": 50})

    def calculate(expression):
        """æ‰§è¡Œæ•°å­¦è®¡ç®— / Execute mathematical calculation"""
        try:
            result = eval(expression)
            return {"result": result, "success": True}
        except Exception as e:
            return {"result": None, "success": False, "error": str(e)}

    def process_tool_call(tool_call):
        """å¤„ç†å·¥å…·è°ƒç”¨ / Process tool call"""
        tool_name = tool_call.name
        tool_args = tool_call.input

        if tool_name == "get_weather":
            result = get_weather(tool_args["city"])
            return f"{tool_args['city']}çš„å¤©æ°”ï¼šæ¸©åº¦{result['temperature']}Â°Cï¼Œ{result['condition']}ï¼Œæ¹¿åº¦{result['humidity']}%"

        elif tool_name == "calculate":
            result = calculate(tool_args["expression"])
            if result["success"]:
                return f"è®¡ç®—ç»“æœï¼š{result['result']}"
            else:
                return f"è®¡ç®—é”™è¯¯ï¼š{result['error']}"

        return "æœªçŸ¥å·¥å…·"

    def chat_with_tools(user_message):
        """å¸¦å·¥å…·çš„å¯¹è¯ / Chat with tools"""

        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0.7,
            system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": user_message
                }
            ],
            tools=tools
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ / Check if there are tool calls
        if message.stop_reason == "tool_use":
            tool_results = []
            for tool_call in message.content:
                if hasattr(tool_call, 'name'):  # è¿™æ˜¯å·¥å…·è°ƒç”¨ / This is a tool call
                    result = process_tool_call(tool_call)
                    tool_results.append(result)

            # ä½¿ç”¨å·¥å…·ç»“æœç»§ç»­å¯¹è¯ / Continue conversation with tool results
            final_message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1000,
                temperature=0.7,
                system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": user_message
                    },
                    {
                        "role": "assistant",
                        "content": message.content
                    },
                    {
                        "role": "user",
                        "content": f"å·¥å…·è°ƒç”¨ç»“æœï¼š{'; '.join(tool_results)}"
                    }
                ]
            )

            return final_message.content[0].text

        return message.content[0].text

    # ä½¿ç”¨ç¤ºä¾‹ / Usage examples
    print("å·¥å…·è°ƒç”¨ç¤ºä¾‹:")
    print(chat_with_tools("åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"))
    print(chat_with_tools("è®¡ç®—25ä¹˜ä»¥17ç­‰äºå¤šå°‘ï¼Ÿ"))

    return chat_with_tools

# æ‰§è¡Œå·¥å…·è°ƒç”¨ç¤ºä¾‹ / Execute tool calling example
tool_chat = basic_tool_calling()
```

### é«˜çº§å·¥å…·é“¾ / Advanced Tool Chain

```python
def advanced_tool_chain():
    """é«˜çº§å·¥å…·é“¾ç¤ºä¾‹ / Advanced tool chain example"""

    # æ›´å¤æ‚çš„å·¥å…·å®šä¹‰ / More complex tool definitions
    advanced_tools = [
        {
            "name": "web_search",
            "description": "åœ¨ç½‘ç»œä¸Šæœç´¢ä¿¡æ¯",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢"},
                    "max_results": {"type": "integer", "description": "æœ€å¤§ç»“æœæ•°é‡", "default": 5}
                },
                "required": ["query"]
            }
        },
        {
            "name": "code_executor",
            "description": "æ‰§è¡ŒPythonä»£ç ",
            "input_schema": {
                "type": "object",
                "properties": {
                    "code": {"type": "string", "description": "è¦æ‰§è¡Œçš„Pythonä»£ç "}
                },
                "required": ["code"]
            }
        },
        {
            "name": "data_analyzer",
            "description": "åˆ†ææ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š",
            "input_schema": {
                "type": "object",
                "properties": {
                    "data": {"type": "string", "description": "è¦åˆ†æçš„æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"},
                    "analysis_type": {"type": "string", "description": "åˆ†æç±»å‹", "enum": ["summary", "correlation", "trend"]}
                },
                "required": ["data", "analysis_type"]
            }
        }
    ]

    def execute_web_search(query, max_results=5):
        """æ‰§è¡Œç½‘ç»œæœç´¢ / Execute web search"""
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ / Simulate search results
        mock_results = [
            f"å…³äº'{query}'çš„ç»“æœ 1",
            f"å…³äº'{query}'çš„ç»“æœ 2",
            f"å…³äº'{query}'çš„ç»“æœ 3"
        ]
        return mock_results[:max_results]

    def execute_code(code):
        """æ‰§è¡ŒPythonä»£ç  / Execute Python code"""
        try:
            # æ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼æ‰§è¡Œä»£ç 
            # Note: In real applications, use safer ways to execute code
            result = eval(code)
            return {"success": True, "result": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def analyze_data(data, analysis_type):
        """åˆ†ææ•°æ® / Analyze data"""
        try:
            import json
            parsed_data = json.loads(data)

            if analysis_type == "summary":
                return f"æ•°æ®æ‘˜è¦ï¼šåŒ…å«{len(parsed_data)}ä¸ªæ•°æ®ç‚¹"
            elif analysis_type == "correlation":
                return "æ•°æ®ç›¸å…³æ€§åˆ†æï¼šå‘ç°å¼ºç›¸å…³æ€§"
            elif analysis_type == "trend":
                return "è¶‹åŠ¿åˆ†æï¼šæ•°æ®å‘ˆä¸Šå‡è¶‹åŠ¿"

        except Exception as e:
            return f"æ•°æ®åˆ†æé”™è¯¯ï¼š{str(e)}"

    def process_advanced_tools(message):
        """å¤„ç†é«˜çº§å·¥å…·è°ƒç”¨ / Process advanced tool calls"""

        tool_calls = []
        for content_block in message.content:
            if hasattr(content_block, 'name'):  # å·¥å…·è°ƒç”¨ / Tool call
                tool_calls.append(content_block)

        if not tool_calls:
            return None

        tool_results = []
        for tool_call in tool_calls:
            if tool_call.name == "web_search":
                result = execute_web_search(**tool_call.input)
                tool_results.append(f"æœç´¢ç»“æœï¼š{result}")

            elif tool_call.name == "code_executor":
                result = execute_code(tool_call.input["code"])
                tool_results.append(f"ä»£ç æ‰§è¡Œç»“æœï¼š{result}")

            elif tool_call.name == "data_analyzer":
                result = analyze_data(**tool_call.input)
                tool_results.append(f"æ•°æ®åˆ†æç»“æœï¼š{result}")

        return tool_results

    def advanced_chat_with_tools(user_query):
        """é«˜çº§å·¥å…·å¯¹è¯ / Advanced tool conversation"""

        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            temperature=0.7,
            system="ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": user_query
                }
            ],
            tools=advanced_tools
        )

        tool_results = process_advanced_tools(message)

        if tool_results:
            # ä½¿ç”¨å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­” / Generate final answer using tool results
            final_message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2000,
                temperature=0.7,
                system="ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å„ç§å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": user_query
                    },
                    {
                        "role": "assistant",
                        "content": message.content
                    },
                    {
                        "role": "user",
                        "content": f"å·¥å…·æ‰§è¡Œç»“æœï¼š\n" + "\n".join(tool_results)
                    }
                ]
            )

            return final_message.content[0].text

        return message.content[0].text

    # é«˜çº§å·¥å…·ä½¿ç”¨ç¤ºä¾‹ / Advanced tool usage examples
    print("é«˜çº§å·¥å…·é“¾ç¤ºä¾‹:")

    # ç¤ºä¾‹1ï¼šç»“åˆæœç´¢å’Œåˆ†æ / Example 1: Combine search and analysis
    query1 = "åˆ†æå½“å‰AIå‘å±•è¶‹åŠ¿ï¼Œå¹¶æœç´¢ç›¸å…³æœ€æ–°ç ”ç©¶ã€‚"
    result1 = advanced_chat_with_tools(query1)
    print(f"æŸ¥è¯¢1ç»“æœ: {result1[:200]}...")

    # ç¤ºä¾‹2ï¼šä»£ç æ‰§è¡Œå’Œæ•°æ®åˆ†æ / Example 2: Code execution and data analysis
    query2 = "è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—å‰10é¡¹ï¼Œå¹¶åˆ†æå…¶å¢é•¿è¶‹åŠ¿ã€‚"
    result2 = advanced_chat_with_tools(query2)
    print(f"æŸ¥è¯¢2ç»“æœ: {result2[:200]}...")

    return advanced_chat_with_tools

# æ‰§è¡Œé«˜çº§å·¥å…·é“¾ç¤ºä¾‹ / Execute advanced tool chain example
advanced_tools = advanced_tool_chain()
```

---

## æµå¼å“åº”å¤„ç† / Streaming Response Handling

### åŸºç¡€æµå¼å“åº” / Basic Streaming Response

```python
import asyncio

def basic_streaming_response():
    """åŸºç¡€æµå¼å“åº”å¤„ç† / Basic streaming response handling"""

    async def stream_response_async(user_message):
        """å¼‚æ­¥æµå¼å“åº” / Asynchronous streaming response"""

        print("å¼€å§‹æµå¼å“åº”... / Starting streaming response...")

        async with client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0.7,
            system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": user_message
                }
            ]
        ) as stream:
            full_response = ""
            chunk_count = 0

            async for chunk in stream:
                if chunk.type == "content_block_delta":
                    content = chunk.delta.text
                    if content:
                        print(content, end="", flush=True)
                        full_response += content
                        chunk_count += 1

                        # æ¯10ä¸ªå—æ‰“å°ä¸€æ¬¡è¿›åº¦ / Print progress every 10 chunks
                        if chunk_count % 10 == 0:
                            print(f"\n[å·²æ¥æ”¶ {chunk_count} ä¸ªå†…å®¹å—] / [Received {chunk_count} content blocks]")

            print(f"\n\næµå¼å“åº”å®Œæˆï¼æ€»å…±æ¥æ”¶äº† {chunk_count} ä¸ªå†…å®¹å—ã€‚")
            print(f"å®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")

            return full_response

    def stream_response_sync(user_message):
        """åŒæ­¥æµå¼å“åº” / Synchronous streaming response"""

        print("å¼€å§‹åŒæ­¥æµå¼å“åº”... / Starting synchronous streaming response...")

        with client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0.7,
            system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": user_message
                }
            ]
        ) as stream:
            full_response = ""
            chunk_count = 0

            for chunk in stream:
                if chunk.type == "content_block_delta":
                    content = chunk.delta.text
                    if content:
                        print(content, end="", flush=True)
                        full_response += content
                        chunk_count += 1

            print(f"\n\nåŒæ­¥æµå¼å“åº”å®Œæˆï¼æ€»å…±æ¥æ”¶äº† {chunk_count} ä¸ªå†…å®¹å—ã€‚")

            return full_response

    # ä½¿ç”¨ç¤ºä¾‹ / Usage examples
    test_message = "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œå¹¶ç»™å‡ºä¸€äº›å®é™…åº”ç”¨æ¡ˆä¾‹ã€‚"

    print("="*60)
    print("å¼‚æ­¥æµå¼å“åº”ç¤ºä¾‹ / Asynchronous Streaming Example")
    print("="*60)

    # å¼‚æ­¥æ‰§è¡Œ / Asynchronous execution
    asyncio.run(stream_response_async(test_message))

    print("\n" + "="*60)
    print("åŒæ­¥æµå¼å“åº”ç¤ºä¾‹ / Synchronous Streaming Example")
    print("="*60)

    # åŒæ­¥æ‰§è¡Œ / Synchronous execution
    stream_response_sync(test_message)

    return {
        "async_streaming": lambda msg: asyncio.run(stream_response_async(msg)),
        "sync_streaming": stream_response_sync
    }

# æ‰§è¡Œæµå¼å“åº”ç¤ºä¾‹ / Execute streaming response example
streaming_functions = basic_streaming_response()
```

### é«˜çº§æµå¼å¤„ç† / Advanced Streaming Processing

```python
def advanced_streaming_processing():
    """é«˜çº§æµå¼å¤„ç†ç¤ºä¾‹ / Advanced streaming processing example"""

    class StreamingProcessor:
        """æµå¼å“åº”å¤„ç†å™¨ / Streaming response processor"""

        def __init__(self):
            self.full_response = ""
            self.chunk_count = 0
            self.start_time = None
            self.end_time = None
            self.processing_stats = {
                "total_chunks": 0,
                "text_chunks": 0,
                "empty_chunks": 0,
                "avg_chunk_size": 0,
                "total_size": 0
            }

        def start_processing(self):
            """å¼€å§‹å¤„ç† / Start processing"""
            self.start_time = time.time()
            print("ğŸš€ å¼€å§‹å¤„ç†æµå¼å“åº”... / Starting streaming response processing...")

        def process_chunk(self, chunk):
            """å¤„ç†å•ä¸ªæ•°æ®å— / Process individual chunk"""
            self.chunk_count += 1
            self.processing_stats["total_chunks"] += 1

            if chunk.type == "content_block_delta":
                content = chunk.delta.text
                if content:
                    print(content, end="", flush=True)
                    self.full_response += content
                    self.processing_stats["text_chunks"] += 1
                    self.processing_stats["total_size"] += len(content)
                else:
                    self.processing_stats["empty_chunks"] += 1

            # å®æ—¶ç»Ÿè®¡æ›´æ–° / Real-time statistics update
            if self.chunk_count % 20 == 0:
                self._print_progress_stats()

        def finish_processing(self):
            """å®Œæˆå¤„ç† / Finish processing"""
            self.end_time = time.time()
            processing_time = self.end_time - self.start_time

            print("
âœ… æµå¼å“åº”å¤„ç†å®Œæˆï¼ / Streaming response processing completed!"            print("ğŸ“Š å¤„ç†ç»Ÿè®¡ / Processing Statistics:"            print(f"  - æ€»æ•°æ®å—æ•°: {self.processing_stats['total_chunks']}")
            print(f"  - æ–‡æœ¬æ•°æ®å—æ•°: {self.processing_stats['text_chunks']}")
            print(f"  - ç©ºæ•°æ®å—æ•°: {self.processing_stats['empty_chunks']}")
            print(f"  - æ€»å­—ç¬¦æ•°: {self.processing_stats['total_size']}")
            if self.processing_stats['text_chunks'] > 0:
                avg_size = self.processing_stats['total_size'] / self.processing_stats['text_chunks']
                print(".1f"            print(".2f"            print(".1f"
            return self.full_response

        def _print_progress_stats(self):
            """æ‰“å°è¿›åº¦ç»Ÿè®¡ / Print progress statistics"""
            elapsed = time.time() - self.start_time
            print(".1f"
    def advanced_streaming_chat(user_message):
        """é«˜çº§æµå¼å¯¹è¯ / Advanced streaming chat"""

        processor = StreamingProcessor()
        processor.start_processing()

        with client.messages.stream(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            temperature=0.7,
            system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·æä¾›è¯¦ç»†è€Œæœ‰å¸®åŠ©çš„å›ç­”ã€‚",
            messages=[
                {
                    "role": "user",
                    "content": user_message
                }
            ]
        ) as stream:
            for chunk in stream:
                processor.process_chunk(chunk)

        return processor.finish_processing()

    # é«˜çº§æµå¼å¤„ç†ç¤ºä¾‹ / Advanced streaming processing example
    print("="*80)
    print("ğŸ¯ é«˜çº§æµå¼å“åº”å¤„ç†ç¤ºä¾‹ / Advanced Streaming Response Processing Example")
    print("="*80)

    test_query = """
    è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    1. äººå·¥æ™ºèƒ½çš„èµ·æºå’Œå‘å±•é˜¶æ®µ
    2. å…³é”®æŠ€æœ¯çªç ´å’Œé‡Œç¨‹ç¢‘äº‹ä»¶
    3. å½“å‰ä¸»è¦ç ”ç©¶æ–¹å‘å’Œå‘å±•è¶‹åŠ¿
    4. å¯¹æœªæ¥ç¤¾ä¼šçš„å½±å“é¢„æµ‹

    è¯·ç”¨æ¸…æ™°çš„ç»“æ„å’Œå…·ä½“çš„ä¾‹å­æ¥é˜è¿°ã€‚
    """

    result = advanced_streaming_chat(test_query)

    print("
ğŸ” å“åº”æ‘˜è¦ / Response Summary:"    print(f"æ€»å­—ç¬¦æ•°: {len(result)}")
    print(f"æ€»æ®µè½æ•°: {result.count(chr(10)) + 1}")
    print(f"åŒ…å«å…³é”®è¯: {'äººå·¥æ™ºèƒ½' in result}, {'æœºå™¨å­¦ä¹ ' in result}, {'æ·±åº¦å­¦ä¹ ' in result}")

    return result

# æ‰§è¡Œé«˜çº§æµå¼å¤„ç† / Execute advanced streaming processing
import time
advanced_result = advanced_streaming_processing()
```

---

## é”™è¯¯å¤„ç†ä¸é‡è¯• / Error Handling and Retry

### åŸºç¡€é”™è¯¯å¤„ç† / Basic Error Handling

```python
import time
from typing import Optional, Dict, Any

def basic_error_handling():
    """åŸºç¡€é”™è¯¯å¤„ç†ç¤ºä¾‹ / Basic error handling example"""

    class ClaudeAPIError(Exception):
        """Claude API é”™è¯¯ç±» / Claude API Error class"""
        def __init__(self, message: str, status_code: Optional[int] = None):
            super().__init__(message)
            self.status_code = status_code

    def safe_api_call(messages: list, **kwargs) -> str:
        """å®‰å…¨çš„APIè°ƒç”¨ / Safe API call"""

        max_retries = kwargs.pop('max_retries', 3)
        retry_delay = kwargs.pop('retry_delay', 1.0)
        backoff_factor = kwargs.pop('backoff_factor', 2.0)

        last_exception = None

        for attempt in range(max_retries):
            try:
                message = client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    messages=messages,
                    **kwargs
                )

                return message.content[0].text

            except anthropic.APIError as e:
                last_exception = e

                # å¤„ç†ä¸åŒçš„APIé”™è¯¯ / Handle different API errors
                if e.status_code == 400:
                    # è¯·æ±‚é”™è¯¯ / Request error
                    raise ClaudeAPIError(f"è¯·æ±‚é”™è¯¯: {e.message}", e.status_code)

                elif e.status_code == 401:
                    # è®¤è¯é”™è¯¯ / Authentication error
                    raise ClaudeAPIError(f"APIå¯†é’¥æ— æ•ˆ: {e.message}", e.status_code)

                elif e.status_code == 429:
                    # é€Ÿç‡é™åˆ¶ / Rate limit
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (backoff_factor ** attempt)
                        print(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•... / Rate limited, waiting {wait_time:.1f} seconds...")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise ClaudeAPIError(f"é€Ÿç‡é™åˆ¶: {e.message}", e.status_code)

                elif e.status_code >= 500:
                    # æœåŠ¡å™¨é”™è¯¯ / Server error
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (backoff_factor ** attempt)
                        print(f"æœåŠ¡å™¨é”™è¯¯ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•... / Server error, waiting {wait_time:.1f} seconds...")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise ClaudeAPIError(f"æœåŠ¡å™¨é”™è¯¯: {e.message}", e.status_code)

                else:
                    # å…¶ä»–é”™è¯¯ / Other errors
                    raise ClaudeAPIError(f"APIé”™è¯¯: {e.message}", e.status_code)

            except Exception as e:
                last_exception = e
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (backoff_factor ** attempt)
                    print(f"æœªçŸ¥é”™è¯¯ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•... / Unknown error, waiting {wait_time:.1f} seconds...")
                    time.sleep(wait_time)
                    continue
                else:
                    raise ClaudeAPIError(f"æœªçŸ¥é”™è¯¯: {str(e)}")

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ / If all retries fail
        raise ClaudeAPIError(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {str(last_exception)}")

    def robust_chat(user_message: str) -> str:
        """å¥å£®çš„å¯¹è¯å‡½æ•° / Robust chat function"""

        messages = [
            {
                "role": "user",
                "content": user_message
            }
        ]

        try:
            response = safe_api_call(
                messages=messages,
                max_tokens=1000,
                temperature=0.7,
                system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚",
                max_retries=3,
                retry_delay=1.0,
                backoff_factor=2.0
            )

            return response

        except ClaudeAPIError as e:
            print(f"APIè°ƒç”¨å¤±è´¥: {e}")
            if e.status_code == 401:
                return "æŠ±æ­‰ï¼ŒAPIå¯†é’¥é…ç½®æœ‰è¯¯ã€‚è¯·æ£€æŸ¥æ‚¨çš„APIå¯†é’¥è®¾ç½®ã€‚ / Sorry, there's an issue with the API key configuration. Please check your API key settings."
            elif e.status_code == 429:
                return "æŠ±æ­‰ï¼Œå½“å‰è¯·æ±‚è¿‡äºé¢‘ç¹ã€‚è¯·ç¨åå†è¯•ã€‚ / Sorry, the request is too frequent. Please try again later."
            else:
                return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åé‡è¯•ã€‚ / Sorry, the service is temporarily unavailable. Please try again later."

    # é”™è¯¯å¤„ç†æµ‹è¯• / Error handling test
    print("åŸºç¡€é”™è¯¯å¤„ç†ç¤ºä¾‹ / Basic Error Handling Example:")

    # æ­£å¸¸è¯·æ±‚ / Normal request
    normal_response = robust_chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")
    print(f"æ­£å¸¸å“åº”: {normal_response[:100]}...")

    # æ¨¡æ‹Ÿé”™è¯¯æƒ…å†µ / Simulate error conditions
    print("\næ¨¡æ‹Ÿé”™è¯¯æƒ…å†µæµ‹è¯• / Simulated Error Condition Tests:")

    # æ— æ•ˆçš„APIå¯†é’¥æµ‹è¯• / Invalid API key test
    try:
        # è¿™é‡Œå¯ä»¥æ¨¡æ‹Ÿä¸åŒçš„é”™è¯¯æƒ…å†µ / Here you can simulate different error conditions
        print("é”™è¯¯å¤„ç†æœºåˆ¶å·²å‡†å¤‡å°±ç»ª / Error handling mechanism is ready")
    except Exception as e:
        print(f"é”™è¯¯å¤„ç†æµ‹è¯•: {e}")

    return robust_chat

# æ‰§è¡ŒåŸºç¡€é”™è¯¯å¤„ç†ç¤ºä¾‹ / Execute basic error handling example
error_handling_chat = basic_error_handling()
```

### é«˜çº§é”™è¯¯å¤„ç†å’Œç›‘æ§ / Advanced Error Handling and Monitoring

```python
import logging
from datetime import datetime, timedelta
from collections import defaultdict

def advanced_error_handling_and_monitoring():
    """é«˜çº§é”™è¯¯å¤„ç†å’Œç›‘æ§ç¤ºä¾‹ / Advanced error handling and monitoring example"""

    # é…ç½®æ—¥å¿— / Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger('ClaudeAPI')

    class APIMonitor:
        """APIç›‘æ§å™¨ / API Monitor"""

        def __init__(self):
            self.metrics = {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "error_types": defaultdict(int),
                "response_times": [],
                "rate_limits": 0
            }
            self.error_window = timedelta(minutes=5)
            self.recent_errors = []

        def record_request(self, success: bool, response_time: float, error_type: str = None):
            """è®°å½•è¯·æ±‚ / Record request"""
            self.metrics["total_requests"] += 1

            if success:
                self.metrics["successful_requests"] += 1
            else:
                self.metrics["failed_requests"] += 1
                if error_type:
                    self.metrics["error_types"][error_type] += 1

            self.metrics["response_times"].append(response_time)

            # æ¸…ç†æ—§çš„é”™è¯¯è®°å½• / Clean old error records
            current_time = datetime.now()
            self.recent_errors = [
                error for error in self.recent_errors
                if current_time - error["timestamp"] < self.error_window
            ]

            if not success:
                self.recent_errors.append({
                    "timestamp": current_time,
                    "error_type": error_type
                })

        def get_health_status(self) -> Dict[str, Any]:
            """è·å–å¥åº·çŠ¶æ€ / Get health status"""
            total = self.metrics["total_requests"]
            success_rate = (self.metrics["successful_requests"] / total * 100) if total > 0 else 0

            avg_response_time = (
                sum(self.metrics["response_times"]) / len(self.metrics["response_times"])
                if self.metrics["response_times"] else 0
            )

            error_rate_5min = len(self.recent_errors) / 5  # æ¯åˆ†é’Ÿé”™è¯¯ç‡ / Errors per minute

            health_status = "healthy"
            if success_rate < 95:
                health_status = "degraded"
            if success_rate < 80 or error_rate_5min > 10:
                health_status = "unhealthy"

            return {
                "status": health_status,
                "success_rate": success_rate,
                "avg_response_time": avg_response_time,
                "error_rate_5min": error_rate_5min,
                "total_requests": total,
                "recent_errors": len(self.recent_errors)
            }

        def should_circuit_break(self) -> bool:
            """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç†”æ–­ / Determine if circuit breaker should be triggered"""
            error_rate_5min = len(self.recent_errors) / 5
            return error_rate_5min > 15  # æ¯åˆ†é’Ÿ15ä¸ªé”™è¯¯è§¦å‘ç†”æ–­ / 15 errors per minute trigger circuit breaker

    class CircuitBreaker:
        """ç†”æ–­å™¨ / Circuit Breaker"""

        def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
            self.failure_threshold = failure_threshold
            self.recovery_timeout = recovery_timeout
            self.failure_count = 0
            self.last_failure_time = None
            self.state = "closed"  # closed, open, half_open

        def call_allowed(self) -> bool:
            """æ£€æŸ¥è°ƒç”¨æ˜¯å¦è¢«å…è®¸ / Check if call is allowed"""
            if self.state == "closed":
                return True
            elif self.state == "open":
                if datetime.now().timestamp() - self.last_failure_time.timestamp() > self.recovery_timeout:
                    self.state = "half_open"
                    return True
                return False
            elif self.state == "half_open":
                return True
            return False

        def record_success(self):
            """è®°å½•æˆåŠŸ / Record success"""
            self.failure_count = 0
            if self.state == "half_open":
                self.state = "closed"

        def record_failure(self):
            """è®°å½•å¤±è´¥ / Record failure"""
            self.failure_count += 1
            self.last_failure_time = datetime.now()

            if self.failure_count >= self.failure_threshold:
                self.state = "open"

    def advanced_safe_api_call(messages: list, monitor: APIMonitor, circuit_breaker: CircuitBreaker, **kwargs) -> str:
        """é«˜çº§å®‰å…¨çš„APIè°ƒç”¨ / Advanced safe API call"""

        if not circuit_breaker.call_allowed():
            raise ClaudeAPIError("ç†”æ–­å™¨å·²æ‰“å¼€ï¼Œæš‚åœæœåŠ¡ / Circuit breaker is open, service paused")

        start_time = time.time()

        try:
            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                messages=messages,
                **kwargs
            )

            response_time = time.time() - start_time
            monitor.record_request(success=True, response_time=response_time)
            circuit_breaker.record_success()

            logger.info(".3f"            return message.content[0].text

        except Exception as e:
            response_time = time.time() - start_time
            error_type = type(e).__name__
            monitor.record_request(success=False, response_time=response_time, error_type=error_type)
            circuit_breaker.record_failure()

            logger.error(f"APIè°ƒç”¨å¤±è´¥: {error_type} - {str(e)}")

            # æ£€æŸ¥å¥åº·çŠ¶æ€ / Check health status
            health = monitor.get_health_status()
            if health["status"] == "unhealthy":
                logger.warning("æœåŠ¡å¥åº·çŠ¶æ€ä¸ä½³ / Service health status is poor")

            raise

    def monitored_chat(user_message: str) -> str:
        """å¸¦ç›‘æ§çš„å¯¹è¯ / Monitored chat"""

        monitor = APIMonitor()
        circuit_breaker = CircuitBreaker()

        messages = [
            {
                "role": "user",
                "content": user_message
            }
        ]

        try:
            response = advanced_safe_api_call(
                messages=messages,
                monitor=monitor,
                circuit_breaker=circuit_breaker,
                max_tokens=1000,
                temperature=0.7,
                system="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
            )

            # æ‰“å°å¥åº·çŠ¶æ€ / Print health status
            health = monitor.get_health_status()
            print("
ğŸ“Š å½“å‰å¥åº·çŠ¶æ€ / Current Health Status:"            print(f"  - çŠ¶æ€: {health['status']}")
            print(".1f"            print(".3f"            print(".1f"
            return response

        except ClaudeAPIError as e:
            print(f"APIè°ƒç”¨å¤±è´¥: {e}")

            # æä¾›é™çº§å“åº” / Provide degraded response
            return "æŠ±æ­‰ï¼Œå½“å‰æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åé‡è¯•ã€‚ / Sorry, the service is temporarily unavailable. Please try again later."

    # é«˜çº§é”™è¯¯å¤„ç†å’Œç›‘æ§æµ‹è¯• / Advanced error handling and monitoring test
    print("="*80)
    print("ğŸ”§ é«˜çº§é”™è¯¯å¤„ç†å’Œç›‘æ§ç¤ºä¾‹ / Advanced Error Handling and Monitoring Example")
    print("="*80)

    test_messages = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ã€‚",
        "äººå·¥æ™ºèƒ½æœ‰å“ªäº›ä¸»è¦åº”ç”¨é¢†åŸŸï¼Ÿ"
    ]

    for i, test_msg in enumerate(test_messages, 1):
        print(f"\n--- æµ‹è¯•æ¶ˆæ¯ {i} / Test Message {i} ---")
        response = monitored_chat(test_msg)
        print(f"å“åº”: {response[:150]}...")

        # æ¨¡æ‹Ÿä¸€äº›å»¶è¿Ÿ / Simulate some delay
        time.sleep(0.5)

    return monitored_chat

# æ‰§è¡Œé«˜çº§é”™è¯¯å¤„ç†å’Œç›‘æ§ / Execute advanced error handling and monitoring
advanced_monitored_chat = advanced_error_handling_and_monitoring()
```

---

## æ€§èƒ½ä¼˜åŒ– / Performance Optimization

### æ¨¡å‹é€‰æ‹©å’Œå‚æ•°è°ƒä¼˜ / Model Selection and Parameter Tuning

```python
def performance_optimization():
    """æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹ / Performance optimization example"""

    def benchmark_models():
        """æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯• / Model performance benchmarking"""

        models_to_test = [
            "claude-3-haiku-20240307",    # æœ€å¿«æ¨¡å‹ / Fastest model
            "claude-3-5-sonnet-20241022", # å¹³è¡¡æ€§èƒ½ / Balanced performance
            "claude-3-opus-20240229"      # æœ€å¼ºæ€§èƒ½ / Strongest performance
        ]

        test_prompts = [
            "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†ã€‚",  # ä¸­ç­‰å¤æ‚åº¦ / Medium complexity
            "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ã€‚",       # åˆ›æ„ä»»åŠ¡ / Creative task
            "åˆ†æå½“å‰å…¨çƒç»æµå½¢åŠ¿ã€‚",     # å¤æ‚åˆ†æ / Complex analysis
        ]

        results = {}

        for model in models_to_test:
            print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model} / Testing model: {model}")
            model_results = {}

            for prompt in test_prompts:
                print(f"  ğŸ“ æµ‹è¯•æç¤º: {prompt[:20]}...")

                # å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼ / Multiple tests for average
                response_times = []
                token_counts = []

                for _ in range(3):  # 3æ¬¡æµ‹è¯• / 3 tests
                    start_time = time.time()

                    try:
                        message = client.messages.create(
                            model=model,
                            max_tokens=500,
                            temperature=0.7,
                            messages=[
                                {
                                    "role": "user",
                                    "content": prompt
                                }
                            ]
                        )

                        response_time = time.time() - start_time
                        response_times.append(response_time)

                        # ä¼°ç®—tokenæ•°é‡ / Estimate token count
                        token_count = len(message.content[0].text) * 0.3  # ç²—ç•¥ä¼°ç®— / Rough estimate
                        token_counts.append(token_count)

                    except Exception as e:
                        print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                        continue

                if response_times:
                    avg_time = sum(response_times) / len(response_times)
                    avg_tokens = sum(token_counts) / len(token_counts)
                    tokens_per_second = avg_tokens / avg_time if avg_time > 0 else 0

                    model_results[prompt[:20]] = {
                        "avg_response_time": avg_time,
                        "avg_tokens": avg_tokens,
                        "tokens_per_second": tokens_per_second
                    }

                    print(".3f"                    print(".1f"                    print(".1f"
            results[model] = model_results

        return results

    def optimize_parameters():
        """å‚æ•°ä¼˜åŒ–ç¤ºä¾‹ / Parameter optimization example"""

        test_prompt = "è¯¦ç»†è§£é‡Šæœºå™¨å­¦ä¹ çš„å·¥ä½œåŸç†ï¼Œå¹¶ç»™å‡ºå®é™…åº”ç”¨æ¡ˆä¾‹ã€‚"
        parameter_combinations = [
            {"temperature": 0.1, "max_tokens": 500},
            {"temperature": 0.7, "max_tokens": 1000},
            {"temperature": 1.0, "max_tokens": 1500},
        ]

        print("\nâš™ï¸ å‚æ•°ä¼˜åŒ–æµ‹è¯• / Parameter Optimization Test")
        print("="*60)

        for i, params in enumerate(parameter_combinations, 1):
            print(f"\næµ‹è¯•ç»„åˆ {i} / Test Combination {i}:")
            print(f"  - æ¸©åº¦: {params['temperature']}")
            print(f"  - æœ€å¤§tokenæ•°: {params['max_tokens']}")

            response_times = []

            for _ in range(3):
                start_time = time.time()

                message = client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    temperature=params["temperature"],
                    max_tokens=params["max_tokens"],
                    messages=[
                        {
                            "role": "user",
                            "content": test_prompt
                        }
                    ]
                )

                response_time = time.time() - start_time
                response_times.append(response_time)

            avg_time = sum(response_times) / len(response_times)
            response_length = len(message.content[0].text)

            print(".3f"            print(f"  - å“åº”é•¿åº¦: {response_length} å­—ç¬¦")

            # è´¨é‡è¯„ä¼° / Quality assessment
            quality_score = assess_response_quality(message.content[0].text)
            print(".2f"
    def assess_response_quality(response_text):
        """è¯„ä¼°å“åº”è´¨é‡ / Assess response quality"""
        # ç®€å•çš„è´¨é‡è¯„ä¼° / Simple quality assessment
        score = 0

        # æ£€æŸ¥ç»“æ„å®Œæ•´æ€§ / Check structural completeness
        if "å·¥ä½œåŸç†" in response_text or "how it works" in response_text:
            score += 20

        # æ£€æŸ¥ç¤ºä¾‹ä¸°å¯Œæ€§ / Check example richness
        if "ä¾‹å¦‚" in response_text or "æ¯”å¦‚" in response_text or "example" in response_text.lower():
            score += 20

        # æ£€æŸ¥é€»è¾‘æ¸…æ™°æ€§ / Check logical clarity
        if "é¦–å…ˆ" in response_text or "å…¶æ¬¡" in response_text or "first" in response_text.lower():
            score += 20

        # æ£€æŸ¥æ·±åº¦ / Check depth
        word_count = len(response_text.split())
        if word_count > 100:
            score += 20
        elif word_count > 50:
            score += 10

        # æ£€æŸ¥ä¸“ä¸šæ€§ / Check professionalism
        technical_terms = ["ç®—æ³•", "æ¨¡å‹", "è®­ç»ƒ", "é¢„æµ‹", "algorithm", "model", "training", "prediction"]
        term_count = sum(1 for term in technical_terms if term in response_text)
        score += min(term_count * 5, 20)

        return score

    # æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–æµ‹è¯• / Execute performance optimization tests
    print("="*80)
    print("ğŸš€ Claudeæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹ / Claude Performance Optimization Example")
    print("="*80)

    # 1. æ¨¡å‹åŸºå‡†æµ‹è¯• / Model benchmarking
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯• / Model Performance Benchmarking")
    benchmark_results = benchmark_models()

    # 2. å‚æ•°ä¼˜åŒ– / Parameter optimization
    optimize_parameters()

    # 3. æ€§èƒ½å»ºè®® / Performance recommendations
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®® / Performance Optimization Recommendations")
    print("-" * 60)
    print("1. æ¨¡å‹é€‰æ‹©ç­–ç•¥ / Model Selection Strategy:")
    print("   â€¢ ç®€å•ä»»åŠ¡: ä½¿ç”¨ Claude 3 Haiku")
    print("   â€¢ ä¸€èˆ¬ä»»åŠ¡: ä½¿ç”¨ Claude 3.5 Sonnet")
    print("   â€¢ å¤æ‚ä»»åŠ¡: ä½¿ç”¨ Claude 3 Opus")
    print()
    print("2. å‚æ•°è°ƒä¼˜å»ºè®® / Parameter Tuning Recommendations:")
    print("   â€¢ äº‹å®æ€§ä»»åŠ¡: æ¸©åº¦ 0.1-0.3")
    print("   â€¢ åˆ›æ„ä»»åŠ¡: æ¸©åº¦ 0.7-0.9")
    print("   â€¢ åˆ†æä»»åŠ¡: æ¸©åº¦ 0.3-0.5")
    print()
    print("3. æ€§èƒ½ç›‘æ§ / Performance Monitoring:")
    print("   â€¢ ç›‘æ§å“åº”æ—¶é—´å’Œtokenä½¿ç”¨é‡")
    print("   â€¢ æ ¹æ®ä½¿ç”¨æ¨¡å¼è°ƒæ•´å‚æ•°")
    print("   â€¢ å®šæœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")

    return {
        "benchmark_results": benchmark_results,
        "optimization_tips": [
            "æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹",
            "è°ƒæ•´æ¸©åº¦å‚æ•°ä»¥å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦",
            "ç›‘æ§å’Œä¼˜åŒ–tokenä½¿ç”¨æ•ˆç‡",
            "ä½¿ç”¨æµå¼å“åº”å‡å°‘æ„ŸçŸ¥å»¶è¿Ÿ"
        ]
    }

# æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹ / Execute performance optimization example
import time
performance_results = performance_optimization()
```

---

## å®é™…åº”ç”¨æ¡ˆä¾‹ / Real-world Application Cases

### æ™ºèƒ½å®¢æœç³»ç»Ÿ / Intelligent Customer Service System

```python
def intelligent_customer_service():
    """æ™ºèƒ½å®¢æœç³»ç»Ÿç¤ºä¾‹ / Intelligent customer service system example"""

    class CustomerServiceAgent:
        """å®¢æœä»£ç† / Customer service agent"""

        def __init__(self):
            self.conversation_history = []
            self.user_profile = {}
            self.intent_classifier = self._load_intent_classifier()

        def _load_intent_classifier(self):
            """åŠ è½½æ„å›¾åˆ†ç±»å™¨ / Load intent classifier"""
            # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„æ„å›¾åˆ†ç±»æ¨¡å‹
            # Here you can integrate more complex intent classification models
            return {
                "product_info": ["äº§å“", "åŠŸèƒ½", "ç‰¹ç‚¹", "ä»·æ ¼"],
                "technical_support": ["é—®é¢˜", "é”™è¯¯", "æ•…éšœ", "ä¿®å¤"],
                "billing": ["è´¦å•", "æ”¯ä»˜", "é€€æ¬¾", "å‘ç¥¨"],
                "general": []
            }

        def classify_intent(self, message):
            """åˆ†ç±»ç”¨æˆ·æ„å›¾ / Classify user intent"""
            message_lower = message.lower()

            for intent, keywords in self.intent_classifier.items():
                if any(keyword in message_lower for keyword in keywords):
                    return intent

            return "general"

        def generate_response(self, user_message, intent):
            """ç”Ÿæˆå›å¤ / Generate response"""

            # æ„å»ºç³»ç»Ÿæç¤º / Build system prompt
            system_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœä»£è¡¨ã€‚ç”¨æˆ·çš„é—®é¢˜ç±»å‹æ˜¯ï¼š{intent}

            è¯·æŒ‰ç…§ä»¥ä¸‹åŸåˆ™å›å¤ï¼š
            1. å‹å¥½ä¸“ä¸šï¼Œä¿æŒç¤¼è²Œ
            2. ç†è§£ç”¨æˆ·é—®é¢˜ï¼Œæä¾›å‡†ç¡®è§£ç­”
            3. å¦‚æœéœ€è¦æŠ€æœ¯æ”¯æŒï¼Œå¼•å¯¼ç”¨æˆ·åˆ°åˆé€‚æ¸ é“
            4. ä¸»åŠ¨æä¾›ç›¸å…³å¸®åŠ©ä¿¡æ¯
            5. å›å¤ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿

            å½“å‰å¯¹è¯å†å²ï¼š
            {self._format_conversation_history()}
            """

            # è°ƒç”¨Claudeç”Ÿæˆå›å¤ / Call Claude to generate response
            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=800,
                temperature=0.7,
                system=system_prompt,
                messages=[
                    {
                        "role": "user",
                        "content": user_message
                    }
                ]
            )

            return message.content[0].text

        def _format_conversation_history(self):
            """æ ¼å¼åŒ–å¯¹è¯å†å² / Format conversation history"""
            if not self.conversation_history:
                return "æ— å†å²å¯¹è¯ / No conversation history"

            formatted = []
            for i, (user_msg, ai_response) in enumerate(self.conversation_history[-3:], 1):  # åªæ˜¾ç¤ºæœ€è¿‘3è½® / Show only last 3 rounds
                formatted.append(f"{i}. ç”¨æˆ·: {user_msg[:50]}...")
                formatted.append(f"   AI: {ai_response[:50]}...")

            return "\n".join(formatted)

        def handle_customer_query(self, user_message):
            """å¤„ç†å®¢æˆ·æŸ¥è¯¢ / Handle customer query"""

            # åˆ†ç±»æ„å›¾ / Classify intent
            intent = self.classify_intent(user_message)

            # ç”Ÿæˆå›å¤ / Generate response
            response = self.generate_response(user_message, intent)

            # æ›´æ–°å¯¹è¯å†å² / Update conversation history
            self.conversation_history.append((user_message, response))

            # è®°å½•ç”¨æˆ·åå¥½ / Record user preferences
            self._update_user_profile(user_message, intent)

            return {
                "response": response,
                "intent": intent,
                "confidence": 0.8,  # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„ç½®ä¿¡åº¦è®¡ç®— / Here you can integrate more complex confidence calculation
                "suggested_actions": self._generate_suggested_actions(intent)
            }

        def _update_user_profile(self, message, intent):
            """æ›´æ–°ç”¨æˆ·ç”»åƒ / Update user profile"""
            # ç®€å•çš„ç”¨æˆ·ç”»åƒæ›´æ–°é€»è¾‘ / Simple user profile update logic
            if intent not in self.user_profile:
                self.user_profile[intent] = 0
            self.user_profile[intent] += 1

        def _generate_suggested_actions(self, intent):
            """ç”Ÿæˆå»ºè®®è¡ŒåŠ¨ / Generate suggested actions"""
            suggestions = {
                "product_info": ["æŸ¥çœ‹äº§å“æ‰‹å†Œ", "é¢„çº¦äº§å“æ¼”ç¤º", "è”ç³»é”€å”®å›¢é˜Ÿ"],
                "technical_support": ["æŸ¥çœ‹å¸¸è§é—®é¢˜", "æäº¤æ”¯æŒå·¥å•", "è”ç³»æŠ€æœ¯æ”¯æŒ"],
                "billing": ["æŸ¥çœ‹è´¦å•è¯¦æƒ…", "æ›´æ–°æ”¯ä»˜ä¿¡æ¯", "ç”³è¯·é€€æ¬¾"],
                "general": ["æµè§ˆå¸®åŠ©ä¸­å¿ƒ", "è”ç³»å®¢æœ", "æŸ¥çœ‹æœ€æ–°æ›´æ–°"]
            }
            return suggestions.get(intent, ["æµè§ˆå¸®åŠ©ä¸­å¿ƒ"])

    # åˆ›å»ºå®¢æœä»£ç† / Create customer service agent
    agent = CustomerServiceAgent()

    # æµ‹è¯•å®¢æœç³»ç»Ÿ / Test customer service system
    test_queries = [
        "ä½ ä»¬çš„è½¯ä»¶æœ‰ä»€ä¹ˆåŠŸèƒ½ï¼Ÿ",
        "æˆ‘çš„è´¦æˆ·å‡ºç°äº†ç™»å½•é—®é¢˜",
        "æˆ‘æƒ³æŸ¥çœ‹ä¸€ä¸‹è´¦å•",
        "ä½ ä»¬çš„äº§å“ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
    ]

    print("ğŸ¤– æ™ºèƒ½å®¢æœç³»ç»Ÿæ¼”ç¤º / Intelligent Customer Service System Demo")
    print("="*80)

    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ·æŸ¥è¯¢ {i}: {query}")
        result = agent.handle_customer_query(query)

        print(f"ğŸ¯ è¯†åˆ«æ„å›¾: {result['intent']}")
        print(".2f"        print(f"ğŸ’¬ AIå›å¤: {result['response'][:100]}...")
        print(f"ğŸ“‹ å»ºè®®è¡ŒåŠ¨: {', '.join(result['suggested_actions'])}")
        print("-" * 50)

    return agent

# æ‰§è¡Œæ™ºèƒ½å®¢æœç³»ç»Ÿ / Execute intelligent customer service system
customer_service_agent = intelligent_customer_service()
```

### ä»£ç ç”Ÿæˆå’Œå®¡æŸ¥åŠ©æ‰‹ / Code Generation and Review Assistant

```python
def code_generation_assistant():
    """ä»£ç ç”Ÿæˆå’Œå®¡æŸ¥åŠ©æ‰‹ç¤ºä¾‹ / Code generation and review assistant example"""

    class CodeAssistant:
        """ä»£ç åŠ©æ‰‹ / Code assistant"""

        def __init__(self):
            self.supported_languages = ["python", "javascript", "java", "cpp", "go"]
            self.code_templates = self._load_code_templates()

        def _load_code_templates(self):
            """åŠ è½½ä»£ç æ¨¡æ¿ / Load code templates"""
            return {
                "python": {
                    "class": """
class {ClassName}:
    def __init__(self, {parameters}):
        {initialization}

    def {method_name}(self, {method_params}):
        {method_body}
""",
                    "function": """
def {function_name}({parameters}):
    \"\"\"
    {docstring}
    \"\"\"
    {function_body}
    return {return_value}
"""
                }
            }

        def generate_code(self, language, task_description, requirements=None):
            """ç”Ÿæˆä»£ç  / Generate code"""

            requirements_text = ""
            if requirements:
                requirements_text = "\n\nå…·ä½“è¦æ±‚ï¼š\n" + "\n".join(f"- {req}" for req in requirements)

            prompt = f"""
            è¯·ç”¨{language}è¯­è¨€ç”Ÿæˆä»£ç ï¼Œå®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š

            ä»»åŠ¡æè¿°ï¼š{task_description}{requirements_text}

            è¦æ±‚ï¼š
            1. ä»£ç è¦å®Œæ•´å¯è¿è¡Œ
            2. åŒ…å«å¿…è¦çš„æ³¨é‡Š
            3. éµå¾ª{language}çš„æœ€ä½³å®è·µ
            4. å¤„ç†å¯èƒ½çš„é”™è¯¯æƒ…å†µ
            5. æä¾›ä½¿ç”¨ç¤ºä¾‹

            è¯·ç›´æ¥è¾“å‡ºä»£ç ï¼Œä¸è¦æ·»åŠ å¤šä½™çš„è§£é‡Šã€‚
            """

            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1500,
                temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„ä»£ç  / Lower temperature for more accurate code
                system=f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{language}å¼€å‘è€…ï¼Œæ“…é•¿ç¼–å†™é«˜è´¨é‡ã€å¯ç»´æŠ¤çš„ä»£ç ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            return message.content[0].text

        def review_code(self, code, language):
            """å®¡æŸ¥ä»£ç  / Review code"""

            prompt = f"""
            è¯·å®¡æŸ¥ä»¥ä¸‹{language}ä»£ç ï¼š

            ```{language}
            {code}
            ```

            è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œå®¡æŸ¥ï¼š
            1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
            2. æ€§èƒ½å’Œæ•ˆç‡
            3. å®‰å…¨æ€§
            4. æœ€ä½³å®è·µéµå¾ªæƒ…å†µ
            5. æ½œåœ¨çš„æ”¹è¿›å»ºè®®

            è¯·æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œä¿®æ”¹åçš„ä»£ç ç¤ºä¾‹ã€‚
            """

            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1500,
                temperature=0.2,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„å®¡æŸ¥ / Lower temperature for more accurate review
                system="ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«ä»£ç é—®é¢˜å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            return message.content[0].text

        def explain_code(self, code, language):
            """è§£é‡Šä»£ç  / Explain code"""

            prompt = f"""
            è¯·è¯¦ç»†è§£é‡Šä»¥ä¸‹{language}ä»£ç çš„åŠŸèƒ½å’Œå·¥ä½œåŸç†ï¼š

            ```{language}
            {code}
            ```

            è¯·åŒ…æ‹¬ï¼š
            1. ä»£ç çš„æ•´ä½“åŠŸèƒ½
            2. å…³é”®éƒ¨åˆ†çš„è¯¦ç»†è§£é‡Š
            3. ä½¿ç”¨çš„æŠ€æœ¯å’Œæ¨¡å¼
            4. å¯èƒ½çš„æ”¹è¿›æ–¹å‘

            è¯·ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è¿›è¡Œè§£é‡Šã€‚
            """

            message = client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1200,
                temperature=0.5,
                system="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹å¯¼å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚çš„ä»£ç ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            return message.content[0].text

    # åˆ›å»ºä»£ç åŠ©æ‰‹ / Create code assistant
    assistant = CodeAssistant()

    # æ¼”ç¤ºä»£ç ç”Ÿæˆ / Demonstrate code generation
    print("ğŸ’» ä»£ç ç”Ÿæˆå’Œå®¡æŸ¥åŠ©æ‰‹æ¼”ç¤º / Code Generation and Review Assistant Demo")
    print("="*80)

    # ç¤ºä¾‹1: ç”ŸæˆPythonå‡½æ•° / Example 1: Generate Python function
    print("\n1ï¸âƒ£ ä»£ç ç”Ÿæˆç¤ºä¾‹ / Code Generation Example")
    print("-" * 50)

    generated_code = assistant.generate_code(
        language="python",
        task_description="å®ç°ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬né¡¹çš„å‡½æ•°",
        requirements=[
            "ä½¿ç”¨é€’å½’æ–¹æ³•",
            "åŒ…å«è¾“å…¥éªŒè¯",
            "æ·»åŠ è¯¦ç»†æ³¨é‡Š",
            "å¤„ç†å¤§æ•°æƒ…å†µ"
        ]
    )

    print("ç”Ÿæˆçš„ä»£ç  / Generated Code:")
    print(generated_code)

    # ç¤ºä¾‹2: ä»£ç å®¡æŸ¥ / Example 2: Code review
    print("\n2ï¸âƒ£ ä»£ç å®¡æŸ¥ç¤ºä¾‹ / Code Review Example")
    print("-" * 50)

    sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

print(fibonacci(10))
"""

    review_result = assistant.review_code(sample_code, "python")
    print("ä»£ç å®¡æŸ¥ç»“æœ / Code Review Result:")
    print(review_result)

    # ç¤ºä¾‹3: ä»£ç è§£é‡Š / Example 3: Code explanation
    print("\n3ï¸âƒ£ ä»£ç è§£é‡Šç¤ºä¾‹ / Code Explanation Example")
    print("-" * 50)

    explanation = assistant.explain_code(sample_code, "python")
    print("ä»£ç è§£é‡Š / Code Explanation:")
    print(explanation)

    return assistant

# æ‰§è¡Œä»£ç ç”Ÿæˆå’Œå®¡æŸ¥åŠ©æ‰‹ / Execute code generation and review assistant
code_assistant = code_generation_assistant()
```

---

## æœ€ä½³å®è·µ / Best Practices

### ğŸ¯ æ ¸å¿ƒæœ€ä½³å®è·µ / Core Best Practices

#### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥ / Model Selection Strategy

**ä»»åŠ¡å¤æ‚åº¦é©±åŠ¨é€‰æ‹© / Task Complexity Driven Selection:**
- **ç®€å•ä»»åŠ¡** (é—®ç­”ã€æ‘˜è¦): ä½¿ç”¨ Claude 3 Haiku
- **ä¸­ç­‰ä»»åŠ¡** (åˆ†æã€å†™ä½œ): ä½¿ç”¨ Claude 3.5 Sonnet
- **å¤æ‚ä»»åŠ¡** (æ¨ç†ã€è§„åˆ’): ä½¿ç”¨ Claude 3 Opus

**æˆæœ¬æ•ˆç›Šè€ƒè™‘ / Cost-Benefit Considerations:**
- è¯„ä¼°ä»»åŠ¡é‡è¦æ€§å’Œé¢‘ç‡
- å¹³è¡¡å“åº”è´¨é‡å’Œæˆæœ¬
- ç›‘æ§APIä½¿ç”¨é‡å’Œè´¹ç”¨

#### 2. æç¤ºå·¥ç¨‹ä¼˜åŒ– / Prompt Engineering Optimization

**ç»“æ„åŒ–æç¤ºè®¾è®¡ / Structured Prompt Design:**
```xml
<optimized_prompt_structure>
  <clear_objective>æ¸…æ™°çš„ç›®æ ‡æè¿°</clear_objective>
  <detailed_context>è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯</detailed_context>
  <specific_requirements>å…·ä½“çš„è´¨é‡è¦æ±‚</specific_requirements>
  <output_format>æ˜ç¡®çš„è¾“å‡ºæ ¼å¼</output_format>
  <examples>ä¸°å¯Œçš„ç¤ºä¾‹è¯´æ˜</examples>
</optimized_prompt_structure>
```

**æç¤ºè¿­ä»£ä¼˜åŒ– / Prompt Iteration Optimization:**
- ä»ç®€å•ç‰ˆæœ¬å¼€å§‹
- åŸºäºç»“æœè¿›è¡Œè¿­ä»£æ”¹è¿›
- A/Bæµ‹è¯•ä¸åŒç‰ˆæœ¬
- æ”¶é›†ç”¨æˆ·åé¦ˆæŒç»­ä¼˜åŒ–

#### 3. é”™è¯¯å¤„ç†å’Œç›‘æ§ / Error Handling and Monitoring

**é˜²å¾¡æ€§ç¼–ç¨‹ / Defensive Programming:**
```python
def robust_api_call(user_input):
    try:
        # è¾“å…¥éªŒè¯ / Input validation
        validated_input = validate_input(user_input)

        # APIè°ƒç”¨ / API call
        response = client.messages.create(...)

        # å“åº”éªŒè¯ / Response validation
        validated_response = validate_response(response)

        return validated_response

    except ValidationError as e:
        logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {e}")
        return get_fallback_response()

    except APIError as e:
        logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
        return get_degraded_response()

    except Exception as e:
        logger.error(f"æœªçŸ¥é”™è¯¯: {e}")
        return get_error_response()
```

**ç›‘æ§å’Œå‘Šè­¦ / Monitoring and Alerting:**
- è®¾ç½®å…³é”®æŒ‡æ ‡ç›‘æ§
- å»ºç«‹å¼‚å¸¸æ£€æµ‹æœºåˆ¶
- é…ç½®è‡ªåŠ¨å‘Šè­¦é€šçŸ¥
- å®šæœŸç”Ÿæˆå¥åº·æŠ¥å‘Š

#### 4. æ€§èƒ½ä¼˜åŒ–å®è·µ / Performance Optimization Practices

**ç¼“å­˜ç­–ç•¥ / Caching Strategy:**
- å®ç°å¤šå±‚ç¼“å­˜ä½“ç³»
- ä½¿ç”¨è¯­ä¹‰ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
- è®¾ç½®åˆç†çš„ç¼“å­˜è¿‡æœŸæ—¶é—´
- ç›‘æ§ç¼“å­˜å‘½ä¸­ç‡

**å¹¶å‘å’Œæ‰¹å¤„ç† / Concurrency and Batching:**
```python
import asyncio
from typing import List

async def batch_process_requests(requests: List[dict]) -> List[dict]:
    """æ‰¹é‡å¤„ç†è¯·æ±‚ / Batch process requests"""

    # åˆ†æ‰¹å¤„ç†é¿å…é€Ÿç‡é™åˆ¶ / Process in batches to avoid rate limits
    batch_size = 10
    results = []

    for i in range(0, len(requests), batch_size):
        batch = requests[i:i + batch_size]

        # å¹¶å‘å¤„ç†æ‰¹æ¬¡ / Process batch concurrently
        tasks = [process_single_request(req) for req in batch]
        batch_results = await asyncio.gather(*tasks)

        results.extend(batch_results)

        # æ‰¹æ¬¡é—´æš‚åœé¿å…è¿‡è½½ / Pause between batches to avoid overload
        await asyncio.sleep(1)

    return results
```

### ğŸ“Š èµ„æºç®¡ç†å’Œæˆæœ¬æ§åˆ¶ / Resource Management and Cost Control

#### APIä½¿ç”¨é‡ç›‘æ§ / API Usage Monitoring

```python
class APIUsageTracker:
    """APIä½¿ç”¨é‡è·Ÿè¸ªå™¨ / API Usage Tracker"""

    def __init__(self):
        self.usage_stats = {
            "daily_requests": 0,
            "monthly_tokens": 0,
            "cost_estimate": 0.0
        }
        self.reset_daily_stats()

    def reset_daily_stats(self):
        """é‡ç½®æ¯æ—¥ç»Ÿè®¡ / Reset daily statistics"""
        self.usage_stats["daily_requests"] = 0
        self.usage_stats["daily_tokens"] = 0
        self.usage_stats["daily_cost"] = 0.0

    def track_request(self, tokens_used: int, model: str):
        """è·Ÿè¸ªè¯·æ±‚ / Track request"""

        self.usage_stats["daily_requests"] += 1
        self.usage_stats["daily_tokens"] += tokens_used

        # ä¼°ç®—æˆæœ¬ / Estimate cost
        cost_per_token = self.get_cost_per_token(model)
        request_cost = tokens_used * cost_per_token
        self.usage_stats["daily_cost"] += request_cost

    def get_cost_per_token(self, model: str) -> float:
        """è·å–æ¯tokenæˆæœ¬ / Get cost per token"""
        # Claudeæ¨¡å‹çš„ä¼°ç®—æˆæœ¬ (ç¾å…ƒ) / Estimated costs for Claude models (USD)
        costs = {
            "claude-3-haiku-20240307": 0.00025,      # è¾“å…¥ / Input
            "claude-3-5-sonnet-20241022": 0.0003,    # è¾“å…¥ / Input
            "claude-3-opus-20240229": 0.0015         # è¾“å…¥ / Input
        }
        return costs.get(model, 0.0003)  # é»˜è®¤ä½¿ç”¨Sonnetçš„ä»·æ ¼ / Default to Sonnet price

    def get_usage_report(self) -> dict:
        """è·å–ä½¿ç”¨æŠ¥å‘Š / Get usage report"""
        return {
            "daily_requests": self.usage_stats["daily_requests"],
            "daily_tokens": self.usage_stats["daily_tokens"],
            "daily_cost_usd": round(self.usage_stats["daily_cost"], 4),
            "avg_tokens_per_request": (
                self.usage_stats["daily_tokens"] / self.usage_stats["daily_requests"]
                if self.usage_stats["daily_requests"] > 0 else 0
            )
        }
```

#### æˆæœ¬ä¼˜åŒ–ç­–ç•¥ / Cost Optimization Strategies

**æ™ºèƒ½æ¨¡å‹é€‰æ‹© / Intelligent Model Selection:**
```python
def select_optimal_model(task_complexity: str, cost_sensitivity: str) -> str:
    """é€‰æ‹©æœ€ä¼˜æ¨¡å‹ / Select optimal model"""

    model_matrix = {
        "simple": {
            "cost_sensitive": "claude-3-haiku-20240307",    # ä¾¿å®œä¸”å¿« / Cheap and fast
            "quality_focused": "claude-3-5-sonnet-20241022" # å¹³è¡¡é€‰æ‹© / Balanced choice
        },
        "complex": {
            "cost_sensitive": "claude-3-5-sonnet-20241022", # æ€§ä»·æ¯”é«˜ / Good value
            "quality_focused": "claude-3-opus-20240229"     # æœ€ä½³è´¨é‡ / Best quality
        }
    }

    return model_matrix.get(task_complexity, {}).get(cost_sensitivity, "claude-3-5-sonnet-20241022")
```

### ğŸ”§ è°ƒè¯•å’Œæ•…éšœæ’é™¤ / Debugging and Troubleshooting

#### å¸¸è§é—®é¢˜è¯Šæ–­ / Common Issue Diagnosis

**ç½‘ç»œè¿æ¥é—®é¢˜ / Network Connection Issues:**
```python
def diagnose_connection_issues():
    """è¯Šæ–­è¿æ¥é—®é¢˜ / Diagnose connection issues"""

    import requests

    try:
        # æµ‹è¯•åŸºæœ¬è¿æ¥ / Test basic connection
        response = requests.get("https://api.anthropic.com", timeout=5)
        print(f"âœ… åŸºæœ¬è¿æ¥æ­£å¸¸: {response.status_code}")

        # æµ‹è¯•APIç«¯ç‚¹ / Test API endpoint
        response = requests.get("https://api.anthropic.com/v1/messages", timeout=5)
        print(f"âœ… APIç«¯ç‚¹å¯è¾¾: {response.status_code}")

    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶ - è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥é”™è¯¯ - è¯·æ£€æŸ¥é˜²ç«å¢™å’Œä»£ç†è®¾ç½®")
    except Exception as e:
        print(f"âŒ æœªçŸ¥è¿æ¥é”™è¯¯: {e}")
```

**APIé”™è¯¯æ’æŸ¥ / API Error Troubleshooting:**

| é”™è¯¯ä»£ç  / Error Code | å¯èƒ½åŸå›  / Possible Causes | è§£å†³æ–¹æ¡ˆ / Solutions |
|----------------------|---------------------------|---------------------|
| 400 | è¯·æ±‚å‚æ•°é”™è¯¯ | æ£€æŸ¥å‚æ•°æ ¼å¼å’Œå€¼ |
| 401 | APIå¯†é’¥æ— æ•ˆ | éªŒè¯å¯†é’¥é…ç½® |
| 429 | è¯·æ±‚è¿‡äºé¢‘ç¹ | å®æ–½æŒ‡æ•°é€€é¿é‡è¯• |
| 500 | æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ | ç­‰å¾…åé‡è¯•ï¼Œæˆ–è”ç³»æ”¯æŒ |
| 529 | æœåŠ¡è¿‡è½½ | å‡å°‘è¯·æ±‚é¢‘ç‡ï¼Œå®æ–½ç†”æ–­ |

---

## ğŸ“… å¼€å‘è¿›åº¦æ—¶é—´è¡¨æ›´æ–°è§„åˆ™ / Development Progress Timestamp Update Rules

> **é“å¾‹ / Iron Rule**: æ¯æ¬¡å¼€å‘æ›´æ–°æ—¶ï¼Œæ—¶é—´è¿›åº¦è¡¨å¿…é¡»ä½¿ç”¨æœ¬æœºç”µè„‘å½“å‰çš„å®æ—¶æ—¥æœŸæ—¶é—´

**æœ€åæ›´æ–° / Last updated: 2025å¹´09æœˆ02æ—¥ 11:35:28**
**æ–‡æ¡£ç‰ˆæœ¬ / Document version: 1.0.0**
**Claudeç³»åˆ—å®ç°ç¤ºä¾‹çŠ¶æ€ / Claude Series Implementation Examples Status: å®Œæˆ / Completed**
