#!/usr/bin/env python3
"""
æœ¬åœ°æ•°æ®å¯¼å…¥å·¥å…·
æ”¯æŒå¤šç§æ ¼å¼çš„æœ¬åœ°æ•°æ®é›†å¯¼å…¥åˆ°MySQLæ•°æ®åº“
"""

import os
import sys
import json
import logging
from typing import Dict, Any, Optional, List, Union
import pandas as pd
from datetime import datetime
import glob
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from src.data.mysql_manager import MySQLManager
from src.config.database_config import DatabaseConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LocalDataImporter:
    """æœ¬åœ°æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(self, mysql_manager: MySQLManager):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨
        
        Args:
            mysql_manager: MySQLç®¡ç†å™¨å®ä¾‹
        """
        self.mysql_manager = mysql_manager
        self.supported_formats = ['.csv', '.xlsx', '.xls', '.json', '.parquet']
        
        # æ•°æ®ç±»å‹æ˜ å°„
        self.dtype_mapping = {
            'object': 'TEXT',
            'int64': 'INTEGER',
            'float64': 'FLOAT',
            'bool': 'BOOLEAN',
            'datetime64[ns]': 'DATETIME'
        }
        
        # é¢„å®šä¹‰è¡¨ç»“æ„æ˜ å°„
        self.table_schemas = {
            'business_data': {
                'date': 'DATETIME',
                'category': 'VARCHAR(100)',
                'region': 'VARCHAR(100)', 
                'gmv': 'FLOAT',
                'dau': 'INTEGER',
                'order_price': 'FLOAT',
                'conversion_rate': 'FLOAT'
            },
            'financial_data': {
                'date': 'DATETIME',
                'transaction_type': 'VARCHAR(50)',
                'currency': 'VARCHAR(10)',
                'amount': 'FLOAT',
                'fee': 'FLOAT',
                'user_id': 'VARCHAR(100)',
                'status': 'VARCHAR(20)'
            },
            'user_data': {
                'username': 'VARCHAR(50)',
                'email': 'VARCHAR(100)',
                'role': 'VARCHAR(20)',
                'is_active': 'INTEGER',
                'created_at': 'DATETIME',
                'last_login': 'DATETIME'
            }
        }
    
    def scan_data_directory(self, directory: str = 'data') -> List[Dict[str, Any]]:
        """
        æ‰«ææ•°æ®ç›®å½•ï¼Œå‘ç°å¯å¯¼å…¥çš„æ•°æ®æ–‡ä»¶
        
        Args:
            directory: æ•°æ®ç›®å½•è·¯å¾„
            
        Returns:
            List: å‘ç°çš„æ•°æ®æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        logger.info(f"ğŸ” æ‰«ææ•°æ®ç›®å½•: {directory}")
        
        found_files = []
        
        # æ‰«ææ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        for ext in self.supported_formats:
            pattern = os.path.join(directory, '**', f'*{ext}')
            files = glob.glob(pattern, recursive=True)
            
            for file_path in files:
                try:
                    file_info = self._analyze_file(file_path)
                    if file_info:
                        found_files.append(file_info)
                except Exception as e:
                    logger.warning(f"åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        
        logger.info(f"âœ… å‘ç° {len(found_files)} ä¸ªå¯å¯¼å…¥æ–‡ä»¶")
        return found_files
    
    def _analyze_file(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        åˆ†ææ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ–‡ä»¶ä¿¡æ¯
        """
        file_ext = Path(file_path).suffix.lower()
        file_size = os.path.getsize(file_path)
        
        # è¯»å–æ–‡ä»¶å¤´éƒ¨æ•°æ®è¿›è¡Œåˆ†æ
        try:
            if file_ext == '.csv':
                df_sample = pd.read_csv(file_path, nrows=5)
            elif file_ext in ['.xlsx', '.xls']:
                df_sample = pd.read_excel(file_path, nrows=5)
            elif file_ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list) and len(data) > 0:
                    df_sample = pd.DataFrame(data[:5])
                else:
                    return None
            elif file_ext == '.parquet':
                df_sample = pd.read_parquet(file_path).head(5)
            else:
                return None
            
            return {
                'file_path': file_path,
                'file_name': os.path.basename(file_path),
                'file_ext': file_ext,
                'file_size': file_size,
                'file_size_mb': round(file_size / 1024 / 1024, 2),
                'columns': list(df_sample.columns),
                'column_count': len(df_sample.columns),
                'sample_data': df_sample.to_dict('records'),
                'suggested_table': self._suggest_table_name(file_path, df_sample.columns)
            }
            
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {str(e)}")
            return None
    
    def _suggest_table_name(self, file_path: str, columns: List[str]) -> str:
        """
        æ ¹æ®æ–‡ä»¶åå’Œåˆ—åå»ºè®®è¡¨å
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            columns: åˆ—ååˆ—è¡¨
            
        Returns:
            str: å»ºè®®çš„è¡¨å
        """
        file_name = Path(file_path).stem.lower()
        
        # æ ¹æ®æ–‡ä»¶ååŒ¹é…
        if any(keyword in file_name for keyword in ['business', 'sales', 'revenue']):
            return 'business_data'
        elif any(keyword in file_name for keyword in ['financial', 'transaction', 'payment']):
            return 'financial_data'
        elif any(keyword in file_name for keyword in ['user', 'customer', 'client']):
            return 'user_data'
        elif any(keyword in file_name for keyword in ['ai', 'agent', 'model']):
            return 'ai_agent_data'
        elif any(keyword in file_name for keyword in ['community', 'group']):
            return 'community_group_buying'
        elif any(keyword in file_name for keyword in ['log', 'system']):
            return 'system_logs'
        
        # æ ¹æ®åˆ—ååŒ¹é…
        columns_str = ' '.join(columns).lower()
        if any(keyword in columns_str for keyword in ['gmv', 'dau', 'conversion']):
            return 'business_data'
        elif any(keyword in columns_str for keyword in ['amount', 'transaction', 'currency']):
            return 'financial_data'
        elif any(keyword in columns_str for keyword in ['username', 'email', 'password']):
            return 'user_data'
        
        # é»˜è®¤ä½¿ç”¨æ–‡ä»¶å
        return file_name.replace('-', '_').replace(' ', '_')
    
    def import_file(self, 
                   file_path: str, 
                   table_name: str = None,
                   chunk_size: int = 1000,
                   if_exists: str = 'append') -> bool:
        """
        å¯¼å…¥å•ä¸ªæ–‡ä»¶åˆ°æ•°æ®åº“
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            table_name: ç›®æ ‡è¡¨å
            chunk_size: åˆ†å—å¤§å°
            if_exists: å¦‚æœè¡¨å­˜åœ¨çš„å¤„ç†æ–¹å¼ ('append', 'replace', 'fail')
            
        Returns:
            bool: å¯¼å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ“¥ å¼€å§‹å¯¼å…¥æ–‡ä»¶: {file_path}")
            
            if not os.path.exists(file_path):
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            # è¯»å–æ•°æ®
            df = self._read_file(file_path)
            if df is None or df.empty:
                logger.error(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶æˆ–æ–‡ä»¶ä¸ºç©º: {file_path}")
                return False
            
            # ç¡®å®šè¡¨å
            if not table_name:
                table_name = self._suggest_table_name(file_path, df.columns)
            
            # æ•°æ®é¢„å¤„ç†
            df_processed = self._preprocess_data(df, table_name)
            
            # å¯¼å…¥æ•°æ®åº“
            success = self._import_to_database(df_processed, table_name, chunk_size, if_exists)
            
            if success:
                logger.info(f"âœ… æ–‡ä»¶å¯¼å…¥æˆåŠŸ: {file_path} -> {table_name} ({len(df_processed)} æ¡è®°å½•)")
                return True
            else:
                logger.error(f"âŒ æ–‡ä»¶å¯¼å…¥å¤±è´¥: {file_path}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False
    
    def _read_file(self, file_path: str) -> Optional[pd.DataFrame]:
        """
        è¯»å–æ–‡ä»¶æ•°æ®
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            DataFrame: è¯»å–çš„æ•°æ®
        """
        file_ext = Path(file_path).suffix.lower()
        
        try:
            if file_ext == '.csv':
                # å°è¯•ä¸åŒç¼–ç 
                for encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                    try:
                        return pd.read_csv(file_path, encoding=encoding)
                    except UnicodeDecodeError:
                        continue
                logger.error(f"æ— æ³•è¯»å–CSVæ–‡ä»¶ï¼Œå°è¯•äº†å¤šç§ç¼–ç : {file_path}")
                return None
                
            elif file_ext in ['.xlsx', '.xls']:
                return pd.read_excel(file_path)
                
            elif file_ext == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list):
                    return pd.DataFrame(data)
                elif isinstance(data, dict):
                    return pd.DataFrame([data])
                else:
                    logger.error(f"ä¸æ”¯æŒçš„JSONæ ¼å¼: {file_path}")
                    return None
                    
            elif file_ext == '.parquet':
                return pd.read_parquet(file_path)
                
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
                return None
                
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None
    
    def _preprocess_data(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """
        æ•°æ®é¢„å¤„ç†
        
        Args:
            df: åŸå§‹æ•°æ®
            table_name: ç›®æ ‡è¡¨å
            
        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®
        """
        df_processed = df.copy()
        
        # æ¸…ç†åˆ—å
        df_processed.columns = [col.strip().replace(' ', '_').replace('-', '_').lower() 
                               for col in df_processed.columns]
        
        # å¤„ç†ç¼ºå¤±å€¼
        df_processed = df_processed.fillna('')
        
        # æ—¥æœŸåˆ—å¤„ç†
        date_columns = ['date', 'created_at', 'updated_at', 'last_login']
        for col in date_columns:
            if col in df_processed.columns:
                try:
                    df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')
                except:
                    pass
        
        # æ·»åŠ æ—¶é—´æˆ³
        if 'created_at' not in df_processed.columns:
            df_processed['created_at'] = datetime.now()
        
        if 'updated_at' not in df_processed.columns:
            df_processed['updated_at'] = datetime.now()
        
        # æ ¹æ®è¡¨ç±»å‹è¿›è¡Œç‰¹å®šå¤„ç†
        if table_name == 'business_data':
            df_processed = self._process_business_data(df_processed)
        elif table_name == 'financial_data':
            df_processed = self._process_financial_data(df_processed)
        elif table_name == 'user_data':
            df_processed = self._process_user_data(df_processed)
        
        return df_processed
    
    def _process_business_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ä¸šåŠ¡æ•°æ®"""
        # ç¡®ä¿æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
        numeric_columns = ['gmv', 'dau', 'order_price', 'conversion_rate']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        return df
    
    def _process_financial_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†é‡‘èæ•°æ®"""
        # ç¡®ä¿é‡‘é¢åˆ—ä¸ºæ•°å€¼ç±»å‹
        numeric_columns = ['amount', 'fee']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        return df
    
    def _process_user_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ç”¨æˆ·æ•°æ®"""
        # å¤„ç†å¸ƒå°”åˆ—
        if 'is_active' in df.columns:
            df['is_active'] = df['is_active'].astype(int)
        
        return df
    
    def _import_to_database(self, 
                           df: pd.DataFrame, 
                           table_name: str, 
                           chunk_size: int,
                           if_exists: str) -> bool:
        """
        å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“
        
        Args:
            df: è¦å¯¼å…¥çš„æ•°æ®
            table_name: è¡¨å
            chunk_size: åˆ†å—å¤§å°
            if_exists: å­˜åœ¨æ—¶çš„å¤„ç†æ–¹å¼
            
        Returns:
            bool: å¯¼å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            engine = self.mysql_manager.db_config.engine
            
            # åˆ†å—å¯¼å…¥å¤§æ–‡ä»¶
            if len(df) > chunk_size:
                logger.info(f"ğŸ“Š å¤§æ–‡ä»¶åˆ†å—å¯¼å…¥ï¼Œæ€»è®¡ {len(df)} æ¡è®°å½•ï¼Œåˆ†å—å¤§å° {chunk_size}")
                
                for i in range(0, len(df), chunk_size):
                    chunk = df.iloc[i:i+chunk_size]
                    chunk.to_sql(table_name, engine, if_exists='append' if i > 0 else if_exists, index=False)
                    logger.info(f"  å¯¼å…¥è¿›åº¦: {min(i+chunk_size, len(df))}/{len(df)} æ¡è®°å½•")
            else:
                df.to_sql(table_name, engine, if_exists=if_exists, index=False)
            
            logger.info(f"âœ… æ•°æ®æˆåŠŸå¯¼å…¥è¡¨ {table_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å¯¼å…¥å¤±è´¥: {str(e)}")
            return False
    
    def import_directory(self, 
                        directory: str = 'data',
                        table_mapping: Dict[str, str] = None,
                        if_exists: str = 'append') -> Dict[str, bool]:
        """
        æ‰¹é‡å¯¼å…¥ç›®å½•ä¸­çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶
        
        Args:
            directory: æ•°æ®ç›®å½•
            table_mapping: æ–‡ä»¶åˆ°è¡¨åçš„æ˜ å°„
            if_exists: å­˜åœ¨æ—¶çš„å¤„ç†æ–¹å¼
            
        Returns:
            Dict: å¯¼å…¥ç»“æœ
        """
        logger.info(f"ğŸ“ å¼€å§‹æ‰¹é‡å¯¼å…¥ç›®å½•: {directory}")
        
        # æ‰«ææ–‡ä»¶
        files = self.scan_data_directory(directory)
        
        results = {}
        
        for file_info in files:
            file_path = file_info['file_path']
            
            # ç¡®å®šè¡¨å
            if table_mapping and file_info['file_name'] in table_mapping:
                table_name = table_mapping[file_info['file_name']]
            else:
                table_name = file_info['suggested_table']
            
            # å¯¼å…¥æ–‡ä»¶
            success = self.import_file(file_path, table_name, if_exists=if_exists)
            results[file_path] = success
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for success in results.values() if success)
        total_count = len(results)
        
        logger.info(f"ğŸ“Š æ‰¹é‡å¯¼å…¥å®Œæˆ: {success_count}/{total_count} ä¸ªæ–‡ä»¶æˆåŠŸå¯¼å…¥")
        
        return results
    
    def create_import_report(self, results: Dict[str, bool]) -> str:
        """
        åˆ›å»ºå¯¼å…¥æŠ¥å‘Š
        
        Args:
            results: å¯¼å…¥ç»“æœ
            
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# æ•°æ®å¯¼å…¥æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {timestamp}

## å¯¼å…¥ç»Ÿè®¡
- æ€»æ–‡ä»¶æ•°: {len(results)}
- æˆåŠŸå¯¼å…¥: {sum(1 for success in results.values() if success)}
- å¯¼å…¥å¤±è´¥: {sum(1 for success in results.values() if not success)}

## è¯¦ç»†ç»“æœ
"""
        
        for file_path, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            report += f"- {os.path.basename(file_path)}: {status}\n"
        
        return report


def interactive_import():
    """äº¤äº’å¼æ•°æ®å¯¼å…¥"""
    print("ğŸ“¥ æœ¬åœ°æ•°æ®å¯¼å…¥å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥MySQLè¿æ¥
    print("ğŸ”§ è¯·å…ˆé…ç½®MySQLè¿æ¥...")
    
    # è·å–æ•°æ®åº“é…ç½®
    host = input("æ•°æ®åº“ä¸»æœº (é»˜è®¤: localhost): ") or "localhost"
    port = input("æ•°æ®åº“ç«¯å£ (é»˜è®¤: 3306): ") or "3306"
    user = input("æ•°æ®åº“ç”¨æˆ·å (é»˜è®¤: root): ") or "root"
    password = input("æ•°æ®åº“å¯†ç : ")
    database = input("æ•°æ®åº“åç§° (é»˜è®¤: analysis_system): ") or "analysis_system"
    
    config = {
        'host': host,
        'port': int(port),
        'user': user,
        'password': password,
        'database': database
    }
    
    # åˆ›å»ºMySQLç®¡ç†å™¨
    mysql_manager = MySQLManager(config)
    if not mysql_manager.setup_database():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
        return
    
    # åˆ›å»ºå¯¼å…¥å™¨
    importer = LocalDataImporter(mysql_manager)
    
    # é€‰æ‹©å¯¼å…¥æ–¹å¼
    print("\nğŸ“‚ é€‰æ‹©å¯¼å…¥æ–¹å¼:")
    print("1. æ‰«æå¹¶å¯¼å…¥æ•´ä¸ªç›®å½•")
    print("2. å¯¼å…¥å•ä¸ªæ–‡ä»¶")
    
    choice = input("è¯·é€‰æ‹© (1-2): ").strip()
    
    if choice == '1':
        # ç›®å½•å¯¼å…¥
        data_dir = input("æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: data): ") or "data"
        
        if not os.path.exists(data_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return
        
        # æ‰«ææ–‡ä»¶
        files = importer.scan_data_directory(data_dir)
        
        if not files:
            print("âŒ æœªå‘ç°å¯å¯¼å…¥çš„æ•°æ®æ–‡ä»¶")
            return
        
        print(f"\nğŸ“‹ å‘ç° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶:")
        for i, file_info in enumerate(files, 1):
            print(f"{i}. {file_info['file_name']} ({file_info['file_size_mb']}MB)")
            print(f"   åˆ—æ•°: {file_info['column_count']}, å»ºè®®è¡¨å: {file_info['suggested_table']}")
        
        if input("\nç¡®è®¤å¯¼å…¥æ‰€æœ‰æ–‡ä»¶ï¼Ÿ(y/n): ").lower() == 'y':
            results = importer.import_directory(data_dir)
            
            # æ˜¾ç¤ºç»“æœ
            print("\nğŸ“Š å¯¼å…¥ç»“æœ:")
            for file_path, success in results.items():
                status = "âœ…" if success else "âŒ"
                print(f"{status} {os.path.basename(file_path)}")
            
            # ä¿å­˜æŠ¥å‘Š
            report = importer.create_import_report(results)
            report_file = f"data_import_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nğŸ“„ å¯¼å…¥æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    elif choice == '2':
        # å•æ–‡ä»¶å¯¼å…¥
        file_path = input("æ–‡ä»¶è·¯å¾„: ")
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
        
        table_name = input("ç›®æ ‡è¡¨å (ç•™ç©ºè‡ªåŠ¨æ¨æµ‹): ") or None
        
        if importer.import_file(file_path, table_name):
            print("âœ… æ–‡ä»¶å¯¼å…¥æˆåŠŸ")
        else:
            print("âŒ æ–‡ä»¶å¯¼å…¥å¤±è´¥")
    
    mysql_manager.close_connection()
    print("\nğŸ‘‹ å¯¼å…¥å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    try:
        interactive_import()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main() 