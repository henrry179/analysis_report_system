#!/usr/bin/env python3
"""
ä¸šåŠ¡åˆ†ææŠ¥å‘Šè‡ªåŠ¨åŒ–ç³»ç»Ÿ - ä¸»ç¨‹åº
ç‰ˆæœ¬: v3.1 Enhanced with Retail Business Intelligence
"""

import os
import sys
import time
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, Tuple, Optional, Any

# æ·»åŠ æºä»£ç è·¯å¾„
sys.path.append('src')

# æ¡ä»¶å¯¼å…¥ï¼Œä¼˜é›…å¤„ç†ç¼ºå¤±ä¾èµ–
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: pandas æœªå®‰è£…ï¼ŒæŸäº›åŠŸèƒ½å°†å—é™")

try:
    from apscheduler.schedulers.background import BackgroundScheduler
    SCHEDULER_AVAILABLE = True
except ImportError:
    SCHEDULER_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: APScheduler æœªå®‰è£…ï¼Œä»»åŠ¡è°ƒåº¦åŠŸèƒ½å°†å—é™")

# æ¡ä»¶å¯¼å…¥åˆ†æå’Œå¯è§†åŒ–æ¨¡å—
try:
    from analysis.metrics_analyzer import MetricsAnalyzer
    ANALYSIS_AVAILABLE = True
except ImportError:
    ANALYSIS_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: åˆ†ææ¨¡å—å¯¼å…¥å¤±è´¥")

try:
    from visualization.chart_generator import ChartGenerator
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: å¯è§†åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥")

try:
    from report.report_generator import ReportGenerator, ReportData
    REPORT_AVAILABLE = True
except ImportError:
    REPORT_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: æŠ¥å‘Šç”Ÿæˆæ¨¡å—å¯¼å…¥å¤±è´¥")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('analysis_report.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AnalysisReportSystem:
    """åˆ†ææŠ¥å‘Šç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, input_file: str, output_dir: str):
        """
        åˆå§‹åŒ–åˆ†ææŠ¥å‘Šç³»ç»Ÿ
        
        Args:
            input_file: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.input_file = input_file
        self.output_dir = output_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å— - å¤„ç†ä¾èµ–ç¼ºå¤±çš„æƒ…å†µ
        if VISUALIZATION_AVAILABLE:
            self.chart_generator = ChartGenerator(os.path.join(output_dir, 'charts'))
        else:
            self.chart_generator = None
            logger.warning("å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨ï¼Œå›¾è¡¨ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
            
        if REPORT_AVAILABLE:
            self.report_generator = ReportGenerator(
                template_dir=os.path.join(os.path.dirname(__file__), '..', 'templates'),
                output_dir=os.path.join(output_dir, 'reports')
            )
        else:
            self.report_generator = None
            logger.warning("æŠ¥å‘Šç”Ÿæˆæ¨¡å—ä¸å¯ç”¨ï¼ŒæŠ¥å‘Šç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
        
        logger.info(f"ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: è¾“å…¥æ–‡ä»¶={input_file}, è¾“å‡ºç›®å½•={output_dir}")
        
    def load_data(self) -> Tuple[Any, Any]:
        """
        åŠ è½½æ•°æ®
        
        Returns:
            å½“å‰æœŸå’Œä¸ŠæœŸæ•°æ®ï¼ˆå¦‚æœpandaså¯ç”¨åˆ™ä¸ºDataFrameï¼Œå¦åˆ™ä¸ºæ¨¡æ‹Ÿå¯¹è±¡ï¼‰
            
        Raises:
            FileNotFoundError: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯
            pd.errors.EmptyDataError: æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼ˆä»…å½“pandaså¯ç”¨æ—¶ï¼‰
        """
        if not PANDAS_AVAILABLE:
            # å¦‚æœpandasä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            logger.warning("Pandasä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®")
            
            class MockDataFrame:
                def __init__(self, data):
                    self.data = data
                
                def empty(self):
                    return len(self.data) == 0
                
                def __getitem__(self, key):
                    return self.data.get(key, [])
                
                def max(self):
                    return "2023-01-01"
            
            current_data = MockDataFrame({
                'date': ['2023-01-01'],
                'gmv': [1000],
                'dau': [100]
            })
            previous_data = MockDataFrame({
                'date': ['2022-12-31'],
                'gmv': [900],
                'dau': [90]
            })
            
            return current_data, previous_data
            
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.input_file):
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {self.input_file}")
            
            # è¯»å–æ•°æ®
            data = pd.read_csv(self.input_file)
            
            # éªŒè¯æ•°æ®åˆ—
            required_columns = ['date', 'category', 'region', 'gmv', 'dau', 'frequency', 
                              'order_price', 'conversion_rate']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
            
            # éªŒè¯æ•°æ®ä¸ä¸ºç©º
            if data.empty:
                raise ValueError("æ•°æ®ä¸ºç©º")
            
            # è·å–æœ€æ–°æ—¥æœŸ
            latest_date = data['date'].max()
            
            # è·å–ä¸Šä¸€æœŸæ—¥æœŸ
            previous_date = data[data['date'] < latest_date]['date'].max()
            
            # åˆ†ç¦»å½“å‰æœŸå’Œä¸ŠæœŸæ•°æ®
            current_data = data[data['date'] == latest_date]
            previous_data = data[data['date'] == previous_date]
            
            logger.info(f"æ•°æ®åŠ è½½æˆåŠŸ: å½“å‰æœŸ={latest_date}, ä¸ŠæœŸ={previous_date}")
            return current_data, previous_data
            
        except Exception as e:
            if PANDAS_AVAILABLE and hasattr(pd, 'errors'):
                if isinstance(e, pd.errors.EmptyDataError):
                    logger.error("æ•°æ®æ–‡ä»¶ä¸ºç©º")
                    raise
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            raise
            
    def perform_predictive_analysis(self, data) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„é¢„æµ‹åˆ†æï¼Œå¤„ç†ä¾èµ–ç¼ºå¤±æƒ…å†µ"""
        if not PANDAS_AVAILABLE:
            logger.warning("Pandasä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ")
            return {
                'predictions': [100, 110, 120],
                'future_dates': ['2023-01-02', '2023-01-03', '2023-01-04']
            }
            
        try:
            from sklearn.linear_model import LinearRegression
            import numpy as np
        except ImportError:
            logger.warning("Scikit-learnä¸å¯ç”¨ï¼Œè¿”å›ç®€å•é¢„æµ‹")
            return {
                'predictions': [100, 110, 120],
                'future_dates': ['2023-01-02', '2023-01-03', '2023-01-04']
            }
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
        if hasattr(data, '__len__') and len(data) == 0:
            return {'predictions': [], 'future_dates': []}
        
        if not hasattr(data, '__getitem__'):
            return {'predictions': [], 'future_dates': []}
            
        try:
            # Prepare data for prediction (e.g., use 'date' as feature for GMV forecasting)
            data['date'] = pd.to_datetime(data['date'])
            data = data.sort_values('date')
            X = (data['date'] - data['date'].min()).dt.days.values.reshape(-1, 1)  # Convert dates to days since start
            y = data['gmv'].values  # Target: GMV
            
            if len(X) > 1:  # Ensure enough data points
                model = LinearRegression()
                model.fit(X, y)
                future_days = 30  # Predict next 30 days
                future_X = np.array(range(X[-1][0] + 1, X[-1][0] + future_days + 1)).reshape(-1, 1)
                predictions = model.predict(future_X)
                return {'predictions': predictions.tolist(), 'future_dates': (data['date'].max() + pd.Timedelta(days=range(1, future_days + 1))).tolist()}
            else:
                return {'predictions': [], 'future_dates': []}  # Not enough data
        except Exception as e:
            logger.warning(f"é¢„æµ‹åˆ†æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ: {str(e)}")
            return {'predictions': [], 'future_dates': []}
    
    def send_alerts(self, analysis_results: Dict[str, Any]) -> None:
        import logging  # Already imported, but ensuring it's used here
        
        # Check for anomalies, e.g., if there's a significant drop in GMV or in predictions
        if 'top_declining_categories' in analysis_results and analysis_results['top_declining_categories']:
            logging.warning(f"Alert: Top declining categories detected: {analysis_results['top_declining_categories']}")
            # In a real scenario, integrate with email: import smtplib; but for now, use logging
        
        if 'predictions' in analysis_results and analysis_results['predictions']['predictions']:
            if any(pred < 0 for pred in analysis_results['predictions']['predictions']):  # Simple threshold
                logging.warning("Alert: Predicted GMV values include potential declines below zero.")
        
            
    def generate_report(self, report_date: Optional[str] = None) -> Dict[str, Dict[str, str]]:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            report_date: æŠ¥å‘Šæ—¥æœŸï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ
            
        Returns:
            ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„å­—å…¸
            
        Raises:
            Exception: æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä»»ä½•é”™è¯¯
        """
        try:
            if report_date is None:
                report_date = datetime.now().strftime('%Y-%m-%d')
                
            logger.info(f"å¼€å§‹ç”ŸæˆæŠ¥å‘Š: æŠ¥å‘Šæ—¥æœŸ={report_date}")
            
            # åŠ è½½æ•°æ®
            current_data, previous_data = self.load_data()
            
            # æ‰§è¡Œåˆ†æ - å¤„ç†åˆ†æå™¨ä¸å¯ç”¨çš„æƒ…å†µ
            if ANALYSIS_AVAILABLE:
                analyzer = MetricsAnalyzer(current_data, previous_data)
                analysis_results = analyzer.analyze()
            else:
                logger.warning("åˆ†ææ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æç»“æœ")
                analysis_results = {
                    'gmv_metrics': {
                        'gmv': type('MockMetric', (), {
                            'change_rate': 0.1,
                            'contribution': 0.5
                        })(),
                        'order_price': type('MockMetric', (), {
                            'change_rate': 0.05,
                            'contribution': 0.3
                        })(),
                        'conversion_rate': type('MockMetric', (), {
                            'change_rate': 0.02,
                            'contribution': 0.2
                        })(),
                        'frequency': type('MockMetric', (), {
                            'change_rate': 0.03,
                            'contribution': 0.15
                        })(),
                        'dau': type('MockMetric', (), {
                            'change_rate': 0.08,
                            'contribution': 0.25
                        })()
                    },
                    'top_declining_categories': ['Category A', 'Category B'],
                    'top_declining_regions': ['Region X', 'Region Y'],
                    'improvement_suggestions': ['æå‡è½¬åŒ–ç‡', 'ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ']
                }
            
            # æ·»åŠ é¢„æµ‹åˆ†æç»“æœ
            analysis_results['predictions'] = self.perform_predictive_analysis(current_data)
            self.send_alerts(analysis_results)
            logger.info("æ•°æ®åˆ†æå®Œæˆï¼ŒåŒ…æ‹¬é¢„æµ‹å’Œè­¦æŠ¥")
            
            # ç”Ÿæˆå›¾è¡¨ - å¤„ç†å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨çš„æƒ…å†µ
            if VISUALIZATION_AVAILABLE:
                chart_paths = self.chart_generator.generate_all_charts(analysis_results)
            else:
                logger.warning("å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
                chart_paths = {
                    'gmv_contribution': 'charts/mock_gmv_chart.png',
                    'trend_analysis': 'charts/mock_trend_chart.png'
                }
            logger.info("å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            
            # ç”ŸæˆæŠ¥å‘Š - å¤„ç†æŠ¥å‘Šæ¨¡å—ä¸å¯ç”¨çš„æƒ…å†µ
            if REPORT_AVAILABLE:
                # å‡†å¤‡æŠ¥å‘Šæ•°æ®
                report_data = ReportData(
                    report_date=report_date,
                    gmv_change_rate=analysis_results['gmv_metrics']['gmv'].change_rate,
                    order_price_change_rate=analysis_results['gmv_metrics']['order_price'].change_rate,
                    order_price_contribution=analysis_results['gmv_metrics']['order_price'].contribution,
                    conversion_rate_change=analysis_results['gmv_metrics']['conversion_rate'].change_rate,
                    conversion_rate_contribution=analysis_results['gmv_metrics']['conversion_rate'].contribution,
                    frequency_change_rate=analysis_results['gmv_metrics']['frequency'].change_rate,
                    frequency_contribution=analysis_results['gmv_metrics']['frequency'].contribution,
                    dau_change_rate=analysis_results['gmv_metrics']['dau'].change_rate,
                    dau_contribution=analysis_results['gmv_metrics']['dau'].contribution,
                    top_declining_categories=analysis_results['top_declining_categories'],
                    top_declining_regions=analysis_results['top_declining_regions'],
                    improvement_suggestions=analysis_results['improvement_suggestions']
                )
                
                report_paths = self.report_generator.generate_all_formats(report_data)
            else:
                logger.warning("æŠ¥å‘Šæ¨¡å—ä¸å¯ç”¨ï¼Œåˆ›å»ºæ¨¡æ‹ŸæŠ¥å‘Šè·¯å¾„")
                report_paths = {
                    'markdown': f'reports/report_{report_date}.md',
                    'html': f'reports/report_{report_date}.html'
                }
            logger.info("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            return {
                'charts': chart_paths,
                'reports': report_paths
            }
            
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            raise

    def schedule_tasks(self):
        """ä»»åŠ¡è°ƒåº¦ - å¤„ç†è°ƒåº¦å™¨ä¸å¯ç”¨çš„æƒ…å†µ"""
        if not SCHEDULER_AVAILABLE:
            logger.warning("APSchedulerä¸å¯ç”¨ï¼Œè·³è¿‡ä»»åŠ¡è°ƒåº¦è®¾ç½®")
            return
            
        scheduler = BackgroundScheduler()
        scheduler.add_job(self.generate_report, 'interval', hours=24)  # Example: Run daily
        scheduler.start()
        logger.info("Task scheduling started: Reports will generate every 24 hours.")
        # Keep the scheduler running in the background

def main():
    """ä¸»å‡½æ•° - å¢å¼ºç‰ˆä¸šåŠ¡åˆ†æç³»ç»Ÿ"""
    print("ğŸš€ ä¸šåŠ¡åˆ†ææŠ¥å‘Šè‡ªåŠ¨åŒ–ç³»ç»Ÿ v3.1 Enhanced")
    print("   ä¸“ä¸šé›¶å”®ä¸šåŠ¡æ™ºèƒ½åˆ†æå¹³å°")
    print("=" * 80)
    
    start_time = time.time()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_output_directories()
    
    # ç³»ç»Ÿç»“æœæ±‡æ€»
    system_results = {
        'timestamp': datetime.now().isoformat(),
        'data_processing': None,
        'analysis_results': None,
        'visualization_results': None,
        'report_results': None,
        'retail_report_results': None,
        'total_files_generated': 0,
        'execution_time': 0
    }
    
    try:
        # 1. æ•°æ®å¤„ç†æ¨¡å—
        print("\nğŸ“Š 1. å¯åŠ¨æ•°æ®å¤„ç†æ¨¡å—...")
        system_results['data_processing'] = run_data_processing()
        
        # 2. åˆ†æå¼•æ“æ¨¡å—
        print("\nğŸ§  2. å¯åŠ¨åˆ†æå¼•æ“æ¨¡å—...")
        system_results['analysis_results'] = run_analysis_engine(system_results['data_processing'])
        
        # 3. å¯è§†åŒ–æ¨¡å—
        print("\nğŸ“ˆ 3. å¯åŠ¨å¯è§†åŒ–æ¨¡å—...")
        system_results['visualization_results'] = run_visualization(system_results['analysis_results'])
        
        # 4. æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ
        print("\nğŸ“„ 4. å¯åŠ¨æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ...")
        system_results['report_results'] = run_intelligent_reporting(
            system_results['data_processing'], 
            system_results['analysis_results']
        )
        
        # 5. é›¶å”®ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆ (æ–°å¢)
        print("\nğŸª 5. å¯åŠ¨é›¶å”®ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆ...")
        system_results['retail_report_results'] = run_retail_business_reporting(
            system_results['data_processing'], 
            system_results['analysis_results']
        )
        
        # 6. ç”Ÿæˆç³»ç»Ÿæ‘˜è¦
        print("\nğŸ“‹ 6. ç”Ÿæˆç³»ç»Ÿæ‘˜è¦...")
        generate_system_summary(system_results)
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè¿è¡Œå‡ºé”™: {e}")
        return None
    
    # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
    end_time = time.time()
    system_results['execution_time'] = end_time - start_time
    
    # ç»Ÿè®¡ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡
    total_files = 0
    for key, result in system_results.items():
        if isinstance(result, dict) and 'files_created' in result:
            total_files += len(result['files_created'])
    
    system_results['total_files_generated'] = total_files
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ‰ ç³»ç»Ÿæ‰§è¡Œå®Œæˆ!")
    print("=" * 80)
    print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {system_results['execution_time']:.2f} ç§’")
    print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶æ€»æ•°: {system_results['total_files_generated']} ä¸ª")
    print(f"ğŸ“Š æ•°æ®å¤„ç†: {'âœ…' if system_results['data_processing'] else 'âŒ'}")
    print(f"ğŸ§  æ™ºèƒ½åˆ†æ: {'âœ…' if system_results['analysis_results'] else 'âŒ'}")  
    print(f"ğŸ“ˆ æ•°æ®å¯è§†åŒ–: {'âœ…' if system_results['visualization_results'] else 'âŒ'}")
    print(f"ğŸ“„ æ™ºèƒ½æŠ¥å‘Š: {'âœ…' if system_results['report_results'] else 'âŒ'}")
    print(f"ğŸª é›¶å”®æŠ¥å‘Š: {'âœ…' if system_results['retail_report_results'] else 'âŒ'}")
    print("=" * 80)
    
    return system_results

def run_data_processing():
    """è¿è¡Œæ•°æ®å¤„ç†æ¨¡å—"""
    try:
        from data.enhanced_data_processor import EnhancedDataProcessor
        from data.sample_data_generator import SampleDataGenerator
        
        # ç”Ÿæˆæ ·æœ¬æ•°æ®
        data_gen = SampleDataGenerator()
        sample_data = data_gen.generate_sample_data()
        print("  âœ… æ ·æœ¬æ•°æ®ç”ŸæˆæˆåŠŸ")
        
        # æ•°æ®å¤„ç†
        processor = EnhancedDataProcessor()
        processed_data, pipeline_results = processor.process_pipeline(sample_data)
        
        print(f"  âœ… æ•°æ®å¤„ç†å®Œæˆ (è€—æ—¶: {pipeline_results['processing_time']:.2f}s)")
        print(f"     - éªŒè¯ç»“æœ: {'é€šè¿‡' if pipeline_results['validation_result']['is_valid'] else 'å¤±è´¥'}")
        print(f"     - æ¸…æ´—æ“ä½œ: {len(pipeline_results['cleaning_report']['operations_performed'])} é¡¹")
        print(f"     - è½¬æ¢æ“ä½œ: {len(pipeline_results['transform_report']['transformations_applied'])} é¡¹")
        
        # å¯¼å‡ºå¤„ç†æ—¥å¿—
        log_file = processor.export_processing_log('output/data/processing_log.json')
        
        return {
            'raw_data': sample_data,
            'processed_data': processed_data,
            'pipeline_results': pipeline_results,
            'files_created': ['output/data/processing_log.json'] if log_file else [],
            'status': 'success'
        }
        
    except Exception as e:
        print(f"  âŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e), 'files_created': []}

def run_analysis_engine(data_processing_results):
    """è¿è¡Œåˆ†æå¼•æ“æ¨¡å—"""
    try:
        from analysis.advanced_analytics_engine import AdvancedAnalyticsEngine
        
        if not data_processing_results or data_processing_results['status'] != 'success':
            print("  âš ï¸  æ•°æ®å¤„ç†æœªæˆåŠŸï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œåˆ†æ")
            processed_data = None
        else:
            processed_data = data_processing_results['processed_data']
        
        engine = AdvancedAnalyticsEngine()
        analysis_results = {}
        
        # æ¨¡æ‹Ÿåˆ†æç‰¹å¾ï¼ˆé€‚ç”¨äºé›¶å”®ä¸šåŠ¡ï¼‰
        features = ['gmv', 'dau', 'conversion_rate', 'avg_order_value', 'frequency']
        
        # ç›¸å…³æ€§åˆ†æ
        correlation_result = engine.correlation_analysis(processed_data, features)
        analysis_results['correlation'] = correlation_result
        print(f"  âœ… ç›¸å…³æ€§åˆ†æå®Œæˆï¼Œå‘ç° {len(correlation_result['strong_correlations'])} ä¸ªå¼ºç›¸å…³å…³ç³»")
        
        # è¶‹åŠ¿åˆ†æ
        trend_result = engine.trend_analysis(processed_data, 'gmv')
        analysis_results['trend'] = trend_result
        print(f"  âœ… è¶‹åŠ¿åˆ†æå®Œæˆï¼Œè¶‹åŠ¿æ–¹å‘: {trend_result['trend_direction']}")
        
        # å¼‚å¸¸æ£€æµ‹
        anomaly_result = engine.anomaly_detection(processed_data, 'gmv')
        analysis_results['anomaly'] = anomaly_result
        print(f"  âœ… å¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œæ£€æµ‹åˆ° {len(anomaly_result['anomalies_detected'])} ä¸ªå¼‚å¸¸å€¼")
        
        # ç”¨æˆ·åˆ†ç¾¤
        segmentation_result = engine.advanced_segmentation(processed_data, features)
        analysis_results['segmentation'] = segmentation_result
        print(f"  âœ… ç”¨æˆ·åˆ†ç¾¤å®Œæˆï¼Œè¯†åˆ«å‡º {len(segmentation_result['segments'])} ä¸ªç¾¤ä½“")
        
        # é˜Ÿåˆ—åˆ†æï¼ˆé›¶å”®ä¸šåŠ¡ç‰¹è‰²ï¼‰
        cohort_result = engine.cohort_analysis(processed_data, 'user_id', 'date', 'gmv')
        analysis_results['cohort'] = cohort_result
        print(f"  âœ… é˜Ÿåˆ—åˆ†æå®Œæˆï¼Œæ´å¯Ÿæ•°é‡: {len(cohort_result['cohort_insights'])}")
        
        return {
            'analysis_results': analysis_results,
            'insights_generated': sum(len(result.get('insights', [])) for result in analysis_results.values()),
            'files_created': [],
            'status': 'success'
        }
        
    except Exception as e:
        print(f"  âŒ åˆ†æå¼•æ“å¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e), 'files_created': []}

def run_visualization(analysis_results):
    """è¿è¡Œå¯è§†åŒ–æ¨¡å—"""
    try:
        from visualization.enhanced_chart_generator import EnhancedChartGenerator
        
        if not analysis_results or analysis_results['status'] != 'success':
            print("  âš ï¸  åˆ†æç»“æœæœªæˆåŠŸï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå¯è§†åŒ–")
            
        generator = EnhancedChartGenerator()
        
        # å‡†å¤‡é›¶å”®ä¸šåŠ¡å›¾è¡¨æ•°æ®
        chart_data = {
            'gmv_trend': {'1æœˆ': 8500000, '2æœˆ': 8750000, '3æœˆ': 9200000, '4æœˆ': 9100000},
            'region_analysis': {'åä¸œä¸€åŒº': 32000000, 'åä¸œäºŒåŒº': 29000000, 'åä¸œä¸‰åŒº': 24000000},
            'category_analysis': {'è‚‰ç¦½è›‹ç±»': 15.5, 'æ°´äº§ç±»': 9.9, 'çŒªè‚‰ç±»': 13.8, 'å†·è—åŠ å·¥': 9.6, 'è”¬èœç±»': 10.5, 'æ°´æœç±»': 9.9},
            'store_metrics': {'å¹³å‡GMV': 5450000, 'åªæ•ˆ': 15200, 'æ—¥å‡äº¤æ˜“': 285},
            'forecast': {
                'ä¸‹æœˆé¢„æµ‹': {'predicted': 9500000, 'confidence': 'high'},
                'å­£åº¦é¢„æµ‹': {'predicted': 28500000, 'confidence': 'medium'}
            }
        }
        
        # åˆ›å»ºé«˜çº§ä»ªè¡¨æ¿
        dashboard_result = generator.create_advanced_dashboard(chart_data, 'output/charts')
        print(f"  âœ… é«˜çº§ä»ªè¡¨æ¿åˆ›å»ºå®Œæˆ")
        
        # åˆ›å»ºä¸“ä¸šå›¾è¡¨
        specialized_charts = ['correlation_heatmap', 'funnel_chart']
        charts_result = generator.create_specialized_charts(chart_data, specialized_charts, 'output/charts')
        print(f"  âœ… ä¸“ä¸šå›¾è¡¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(charts_result['charts_created'])} ä¸ª")
        
        files_created = []
        files_created.extend(dashboard_result.get('files_generated', []))
        files_created.extend(charts_result.get('files_generated', []))
        
        return {
            'dashboard_result': dashboard_result,
            'charts_result': charts_result,
            'total_charts': dashboard_result['summary']['total_charts'] + len(charts_result['charts_created']),
            'files_created': files_created,
            'status': 'success'
        }
        
    except Exception as e:
        print(f"  âŒ å¯è§†åŒ–æ¨¡å—å¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e), 'files_created': []}

def run_intelligent_reporting(data_processing_results, analysis_results):
    """è¿è¡Œæ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ"""
    try:
        from reports.intelligent_report_generator import IntelligentReportGenerator
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®
        report_data = {
            'total_gmv': 85000000,
            'total_dau': 43000,
            'conversion_rate': 3.2,
            'regions': ['åä¸œä¸€åŒº', 'åä¸œäºŒåŒº', 'åä¸œä¸‰åŒº'],
            'categories': ['è‚‰ç¦½è›‹ç±»', 'æ°´äº§ç±»', 'çŒªè‚‰ç±»', 'å†·è—åŠ å·¥', 'è”¬èœç±»', 'æ°´æœç±»']
        }
        
        # å‡†å¤‡åˆ†æç»“æœ
        analysis_data = analysis_results.get('analysis_results', {}) if analysis_results and analysis_results['status'] == 'success' else {}
        
        generator = IntelligentReportGenerator()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_result = generator.generate_comprehensive_report(
            report_data, 
            analysis_data,
            'output/reports',
            formats=['html', 'markdown', 'json', 'executive']
        )
        
        print(f"  âœ… æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print(f"     - æŠ¥å‘Šæ ¼å¼: {len(report_result['reports_generated'])} ç§")
        print(f"     - æ´å¯Ÿæ•°é‡: {report_result['insights_count']} ä¸ª")
        print(f"     - å»ºè®®æ•°é‡: {report_result['recommendations_count']} ä¸ª")
        
        return {
            'report_result': report_result,
            'files_created': report_result['files_created'],
            'status': 'success'
        }
        
    except Exception as e:
        print(f"  âŒ æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e), 'files_created': []}

def run_retail_business_reporting(data_processing_results, analysis_results):
    """è¿è¡Œé›¶å”®ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆ (æ–°å¢åŠŸèƒ½)"""
    try:
        from reports.retail_business_report_generator import RetailBusinessReportGenerator
        
        # å‡†å¤‡é›¶å”®ä¸šåŠ¡æ•°æ®
        retail_data = {
            'total_gmv': 850000000,  # 8.5äº¿
            'regions': {
                'åä¸œä¸€åŒº': {'gmv': 320000000, 'stores': 52},
                'åä¸œäºŒåŒº': {'gmv': 290000000, 'stores': 48},
                'åä¸œä¸‰åŒº': {'gmv': 240000000, 'stores': 56}
            },
            'categories': {
                'è‚‰ç¦½è›‹ç±»': {'sales': 132000000, 'growth': 14.0},
                'æ°´äº§ç±»': {'sales': 84000000, 'growth': -6.6},
                'çŒªè‚‰ç±»': {'sales': 117000000, 'growth': -10.7},
                'å†·è—åŠ å·¥': {'sales': 82000000, 'growth': 13.7},
                'è”¬èœç±»': {'sales': 89000000, 'growth': -7.8},
                'æ°´æœç±»': {'sales': 84000000, 'growth': 8.9}
            },
            'store_metrics': {
                'total_stores': 156,
                'avg_gmv_per_store': 5450000,
                'sales_per_sqm': 15200
            }
        }
        
        # å‡†å¤‡åˆ†æç»“æœ
        retail_analysis = analysis_results.get('analysis_results', {}) if analysis_results and analysis_results['status'] == 'success' else {}
        
        generator = RetailBusinessReportGenerator()
        
        # ç”Ÿæˆé›¶å”®ä¸šåŠ¡æŠ¥å‘Š
        retail_report_result = generator.generate_retail_business_report(
            retail_data,
            retail_analysis,
            "2024å¹´3æœˆæ•°æ®(3.1~3.31)",
            'output/reports'
        )
        
        print(f"  âœ… é›¶å”®ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        print(f"     - æŠ¥å‘Šæ ¼å¼: {len(retail_report_result['reports_generated'])} ç§")
        print(f"     - æ–‡ä»¶æ•°é‡: {retail_report_result['report_summary']['total_files']} ä¸ª")
        print(f"     - æŠ¥å‘Šç¼–å·: {retail_report_result['report_summary']['report_id']}")
        
        return {
            'retail_report_result': retail_report_result,
            'files_created': retail_report_result['files_created'],
            'status': 'success'
        }
        
    except Exception as e:
        print(f"  âŒ é›¶å”®ä¸šåŠ¡æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return {'status': 'error', 'error': str(e), 'files_created': []}

def generate_system_summary(system_results):
    """ç”Ÿæˆç³»ç»Ÿæ‰§è¡Œæ‘˜è¦"""
    try:
        summary_content = f"""# ğŸš€ ç³»ç»Ÿæ‰§è¡Œæ‘˜è¦

## ğŸ“Š æ‰§è¡Œæ¦‚å†µ
- **æ‰§è¡Œæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ€»è€—æ—¶**: {system_results['execution_time']:.2f} ç§’
- **ç”Ÿæˆæ–‡ä»¶**: {system_results['total_files_generated']} ä¸ª

## ğŸ“‹ æ¨¡å—æ‰§è¡ŒçŠ¶æ€

### ğŸ“Š æ•°æ®å¤„ç†æ¨¡å—
- **çŠ¶æ€**: {'âœ… æˆåŠŸ' if system_results['data_processing'] and system_results['data_processing']['status'] == 'success' else 'âŒ å¤±è´¥'}
- **å¤„ç†æ—¶é—´**: {system_results['data_processing']['pipeline_results']['processing_time']:.2f}s (å¦‚æœæˆåŠŸ)

### ğŸ§  åˆ†æå¼•æ“æ¨¡å—  
- **çŠ¶æ€**: {'âœ… æˆåŠŸ' if system_results['analysis_results'] and system_results['analysis_results']['status'] == 'success' else 'âŒ å¤±è´¥'}
- **æ´å¯Ÿç”Ÿæˆ**: {system_results['analysis_results']['insights_generated'] if system_results['analysis_results'] and system_results['analysis_results']['status'] == 'success' else 0} ä¸ª

### ğŸ“ˆ å¯è§†åŒ–æ¨¡å—
- **çŠ¶æ€**: {'âœ… æˆåŠŸ' if system_results['visualization_results'] and system_results['visualization_results']['status'] == 'success' else 'âŒ å¤±è´¥'}
- **å›¾è¡¨æ•°é‡**: {system_results['visualization_results']['total_charts'] if system_results['visualization_results'] and system_results['visualization_results']['status'] == 'success' else 0} ä¸ª

### ğŸ“„ æ™ºèƒ½æŠ¥å‘Šæ¨¡å—
- **çŠ¶æ€**: {'âœ… æˆåŠŸ' if system_results['report_results'] and system_results['report_results']['status'] == 'success' else 'âŒ å¤±è´¥'}
- **æŠ¥å‘Šæ ¼å¼**: {len(system_results['report_results']['report_result']['reports_generated']) if system_results['report_results'] and system_results['report_results']['status'] == 'success' else 0} ç§

### ğŸª é›¶å”®ä¸šåŠ¡æŠ¥å‘Šæ¨¡å— (æ–°å¢)
- **çŠ¶æ€**: {'âœ… æˆåŠŸ' if system_results['retail_report_results'] and system_results['retail_report_results']['status'] == 'success' else 'âŒ å¤±è´¥'}
- **æŠ¥å‘Šæ ¼å¼**: {len(system_results['retail_report_results']['retail_report_result']['reports_generated']) if system_results['retail_report_results'] and system_results['retail_report_results']['status'] == 'success' else 0} ç§

## ğŸ“ ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨

### ğŸ“Š æ•°æ®æ–‡ä»¶
{_format_file_list(system_results['data_processing']['files_created'] if system_results['data_processing'] else [])}

### ğŸ“ˆ å›¾è¡¨æ–‡ä»¶  
{_format_file_list(system_results['visualization_results']['files_created'] if system_results['visualization_results'] else [])}

### ğŸ“„ æŠ¥å‘Šæ–‡ä»¶
{_format_file_list(system_results['report_results']['files_created'] if system_results['report_results'] else [])}

### ğŸª é›¶å”®æŠ¥å‘Šæ–‡ä»¶
{_format_file_list(system_results['retail_report_results']['files_created'] if system_results['retail_report_results'] else [])}

## ğŸ¯ ç³»ç»Ÿç‰¹è‰²

### ğŸŒŸ æ ¸å¿ƒäº®ç‚¹
- âœ… **é›¶ä¾èµ–è¿è¡Œ**: æ— éœ€å¤–éƒ¨ä¾èµ–å³å¯æ­£å¸¸å·¥ä½œ
- âœ… **æ¸è¿›å¼å¢å¼º**: æ ¹æ®ä¾èµ–å¯ç”¨æ€§è‡ªåŠ¨å‡çº§åŠŸèƒ½  
- âœ… **æ™ºèƒ½åˆ†æ**: AIé©±åŠ¨çš„æ•°æ®åˆ†æå’Œæ´å¯Ÿç”Ÿæˆ
- âœ… **é›¶å”®ä¸“ä¸š**: ä¸“é—¨é’ˆå¯¹é›¶å”®è¡Œä¸šçš„ä¸šåŠ¡åˆ†ææŠ¥å‘Š
- âœ… **å¤šæ ¼å¼è¾“å‡º**: HTMLã€Markdownã€JSONã€CSVå¤šç§æ ¼å¼

### ğŸ“Š æ€§èƒ½æŒ‡æ ‡
- **å¯åŠ¨é€Ÿåº¦**: {system_results['execution_time']:.2f}ç§’
- **æ–‡ä»¶ç”Ÿæˆé€Ÿåº¦**: {system_results['total_files_generated'] / max(system_results['execution_time'], 0.1):.1f} æ–‡ä»¶/ç§’
- **æˆåŠŸç‡**: {sum(1 for key in ['data_processing', 'analysis_results', 'visualization_results', 'report_results', 'retail_report_results'] if system_results[key] and system_results[key].get('status') == 'success') / 5 * 100:.0f}%

---

*ğŸ¤– æœ¬æ‘˜è¦ç”±ä¸šåŠ¡åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ | ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æ‘˜è¦æ–‡ä»¶
        summary_file = f"output/system_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        print(f"  âœ… ç³»ç»Ÿæ‘˜è¦å·²ç”Ÿæˆ: {summary_file}")
        
    except Exception as e:
        print(f"  âŒ ç³»ç»Ÿæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")

def _format_file_list(files):
    """æ ¼å¼åŒ–æ–‡ä»¶åˆ—è¡¨"""
    if not files:
        return "- æ— æ–‡ä»¶ç”Ÿæˆ"
    
    return "\n".join([f"- {file}" for file in files])

def ensure_output_directories():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    directories = [
        'output',
        'output/reports',
        'output/charts', 
        'output/data',
        'output/test_results'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

if __name__ == "__main__":
    main() 