#!/usr/bin/env python3
"""
Qwen Coder - é€šä¹‰åƒé—®ä»£ç ç”Ÿæˆæ¨¡å‹
Windowsæœ¬åœ°è¿è¡Œç‰ˆæœ¬

åŠŸèƒ½ç‰¹æ€§:
- ä»£ç ç”Ÿæˆå’Œè¡¥å…¨
- ä»£ç è§£é‡Šå’Œä¼˜åŒ–
- å¤šè¯­è¨€æ”¯æŒ
- äº¤äº’å¼å¯¹è¯ç•Œé¢

ä½œè€…: QWCoder Team
ç‰ˆæœ¬: 1.0.0
"""

import os
import sys
import argparse
import platform
from pathlib import Path

# æ£€æŸ¥Pythonç‰ˆæœ¬
if sys.version_info < (3, 8):
    print("âŒ éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
    sys.exit(1)

# æ£€æŸ¥æ“ä½œç³»ç»Ÿ
if platform.system() != "Windows":
    print("âš ï¸  æ­¤è„šæœ¬é’ˆå¯¹Windowsä¼˜åŒ–ï¼Œå…¶ä»–ç³»ç»Ÿå¯èƒ½éœ€è¦è°ƒæ•´")

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")

    required_packages = [
        'torch', 'transformers', 'accelerate',
        'numpy', 'gradio', 'streamlit'
    ]

    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ“ {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"âœ— {package}")

    if missing_packages:
        print(f"\nâŒ ç¼ºå°‘å¿…è¦çš„åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False

    print("âœ… æ‰€æœ‰ä¾èµ–æ£€æŸ¥é€šè¿‡")
    return True

def download_model(model_name="Qwen/Qwen2.5-Coder-7B-Instruct"):
    """ä¸‹è½½Qwen Coderæ¨¡å‹"""
    print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
    print("âš ï¸  æ³¨æ„: æ¨¡å‹æ–‡ä»¶è¾ƒå¤§(çº¦15GB)ï¼Œä¸‹è½½éœ€è¦æ—¶é—´")

    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        print("æ­£åœ¨ä¸‹è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

        print("æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True  # ä½¿ç”¨8-bité‡åŒ–èŠ‚çœå†…å­˜
        )

        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
        return tokenizer, model

    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å»ºè®®:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´(>20GB)")
        print("   3. è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹ç‰ˆæœ¬")
        return None, None

def create_web_interface(tokenizer, model):
    """åˆ›å»ºGradio Webç•Œé¢"""
    print("ğŸŒ åˆ›å»ºWebç•Œé¢...")

    try:
        import gradio as gr

        def generate_code(prompt, max_length=512, temperature=0.7):
            """ç”Ÿæˆä»£ç """
            try:
                inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=max_length,
                        temperature=temperature,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )

                generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return generated_code

            except Exception as e:
                return f"ç”Ÿæˆå¤±è´¥: {e}"

        def explain_code(code):
            """è§£é‡Šä»£ç """
            prompt = f"è¯·è§£é‡Šä»¥ä¸‹ä»£ç çš„åŠŸèƒ½å’Œé€»è¾‘:\n\n{code}\n\nè§£é‡Š:"
            return generate_code(prompt)

        def optimize_code(code, language):
            """ä¼˜åŒ–ä»£ç """
            prompt = f"è¯·ä¼˜åŒ–ä»¥ä¸‹{language}ä»£ç ï¼Œæé«˜æ€§èƒ½å’Œå¯è¯»æ€§:\n\n{code}\n\nä¼˜åŒ–åçš„ä»£ç :"
            return generate_code(prompt)

        # åˆ›å»ºç•Œé¢
        with gr.Blocks(title="Qwen Coder - ä»£ç ç”ŸæˆåŠ©æ‰‹") as interface:
            gr.Markdown("# ğŸ¤– Qwen Coder - é€šä¹‰åƒé—®ä»£ç ç”ŸæˆåŠ©æ‰‹")
            gr.Markdown("æ”¯æŒä»£ç ç”Ÿæˆã€è§£é‡Šã€ä¼˜åŒ–ç­‰å¤šç§åŠŸèƒ½")

            with gr.Tab("ä»£ç ç”Ÿæˆ"):
                prompt_input = gr.Textbox(
                    label="è¾“å…¥éœ€æ±‚æè¿°",
                    placeholder="ä¾‹å¦‚: å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
                    lines=3
                )
                max_length_slider = gr.Slider(
                    minimum=100, maximum=2048, value=512, step=50,
                    label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
                )
                temperature_slider = gr.Slider(
                    minimum=0.1, maximum=1.0, value=0.7, step=0.1,
                    label="åˆ›é€ æ€§ (æ¸©åº¦)"
                )
                generate_btn = gr.Button("ğŸš€ ç”Ÿæˆä»£ç ")
                output_code = gr.Code(label="ç”Ÿæˆçš„ä»£ç ", language="python")

                generate_btn.click(
                    generate_code,
                    inputs=[prompt_input, max_length_slider, temperature_slider],
                    outputs=output_code
                )

            with gr.Tab("ä»£ç è§£é‡Š"):
                code_input = gr.Code(label="è¾“å…¥ä»£ç ", language="python")
                explain_btn = gr.Button("ğŸ” è§£é‡Šä»£ç ")
                explanation_output = gr.Textbox(label="ä»£ç è§£é‡Š", lines=10)

                explain_btn.click(explain_code, inputs=code_input, outputs=explanation_output)

            with gr.Tab("ä»£ç ä¼˜åŒ–"):
                opt_code_input = gr.Code(label="è¾“å…¥éœ€è¦ä¼˜åŒ–çš„ä»£ç ", language="python")
                language_selector = gr.Dropdown(
                    ["Python", "JavaScript", "Java", "C++", "Go"],
                    label="ç¼–ç¨‹è¯­è¨€",
                    value="Python"
                )
                optimize_btn = gr.Button("âš¡ ä¼˜åŒ–ä»£ç ")
                optimized_output = gr.Code(label="ä¼˜åŒ–åçš„ä»£ç ", language="python")

                optimize_btn.click(
                    optimize_code,
                    inputs=[opt_code_input, language_selector],
                    outputs=optimized_output
                )

        return interface

    except ImportError:
        print("âŒ Gradioæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install gradio")
        return None

def create_streamlit_app(tokenizer, model):
    """åˆ›å»ºStreamlitåº”ç”¨"""
    print("ğŸ“± åˆ›å»ºStreamlitåº”ç”¨...")

    streamlit_code = '''import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

st.set_page_config(page_title="Qwen Coder", page_icon="ğŸ¤–")

st.title("ğŸ¤– Qwen Coder - ä»£ç ç”ŸæˆåŠ©æ‰‹")
st.markdown("åŸºäºé€šä¹‰åƒé—®Qwen2.5-Coderæ¨¡å‹")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    max_length = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 100, 2048, 512)
    temperature = st.slider("åˆ›é€ æ€§", 0.1, 1.0, 0.7)

# ä¸»ç•Œé¢
tab1, tab2, tab3 = st.tabs(["ğŸ’» ä»£ç ç”Ÿæˆ", "ğŸ” ä»£ç è§£é‡Š", "âš¡ ä»£ç ä¼˜åŒ–"])

with tab1:
    st.header("ä»£ç ç”Ÿæˆ")
    prompt = st.text_area("æè¿°ä½ çš„éœ€æ±‚", height=100,
                         placeholder="ä¾‹å¦‚: å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—")

    if st.button("ğŸš€ ç”Ÿæˆä»£ç ", type="primary"):
        if prompt:
            with st.spinner("æ­£åœ¨ç”Ÿæˆä»£ç ..."):
                try:
                    # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨¡å‹æ¨ç†
                    st.code("print('Hello, World!')", language="python")
                except Exception as e:
                    st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        else:
            st.warning("è¯·è¾“å…¥éœ€æ±‚æè¿°")

with tab2:
    st.header("ä»£ç è§£é‡Š")
    code_to_explain = st.code("def hello():\n    print('Hello, World!')", language="python")

    if st.button("ğŸ” è§£é‡Šä»£ç "):
        with st.spinner("æ­£åœ¨åˆ†æä»£ç ..."):
            st.write("è¿™æ˜¯ä¸€ä¸ªç®€å•çš„Pythonå‡½æ•°ï¼Œç”¨äºè¾“å‡º'Hello, World!'")

with tab3:
    st.header("ä»£ç ä¼˜åŒ–")
    code_to_optimize = st.text_area("è¾“å…¥éœ€è¦ä¼˜åŒ–çš„ä»£ç ", height=200)
    language = st.selectbox("ç¼–ç¨‹è¯­è¨€", ["Python", "JavaScript", "Java", "C++", "Go"])

    if st.button("âš¡ ä¼˜åŒ–ä»£ç "):
        with st.spinner("æ­£åœ¨ä¼˜åŒ–ä»£ç ..."):
            st.code(code_to_optimize, language=language.lower())
'''

    with open("qwen_coder_app.py", "w", encoding="utf-8") as f:
        f.write(streamlit_code)

    print("âœ… Streamlitåº”ç”¨åˆ›å»ºå®Œæˆ: qwen_coder_app.py")
    print("è¿è¡Œå‘½ä»¤: streamlit run qwen_coder_app.py")

def interactive_mode(tokenizer, model):
    """äº¤äº’å¼å‘½ä»¤è¡Œæ¨¡å¼"""
    print("ğŸ’¬ è¿›å…¥äº¤äº’å¼æ¨¡å¼")
    print("è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("è¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
    print("-" * 50)

    while True:
        try:
            user_input = input("\nğŸ¤– è¯·è¾“å…¥ä½ çš„é—®é¢˜æˆ–éœ€æ±‚: ").strip()

            if user_input.lower() in ['exit', 'quit']:
                print("ğŸ‘‹ å†è§!")
                break

            elif user_input.lower() == 'help':
                print_help()
                continue

            elif not user_input:
                continue

            # ç”Ÿæˆå“åº”
            print("\nğŸ¤” æ­£åœ¨æ€è€ƒ...")
            try:
                inputs = tokenizer(user_input, return_tensors="pt").to(model.device)

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=512,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )

                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                print(f"\nğŸ’¡ å›ç­”:\n{response}")

            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except EOFError:
            print("\nğŸ‘‹ å†è§!")
            break

def print_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    help_text = """
ğŸ¤– Qwen Coder ä½¿ç”¨å¸®åŠ©

å¯ç”¨å‘½ä»¤:
  help        - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  exit/quit   - é€€å‡ºç¨‹åº

åŠŸèƒ½è¯´æ˜:
  ğŸ’» ä»£ç ç”Ÿæˆ - æè¿°éœ€æ±‚å³å¯ç”Ÿæˆç›¸åº”ä»£ç 
  ğŸ” ä»£ç è§£é‡Š - è¾“å…¥ä»£ç ç‰‡æ®µè·å–è¯¦ç»†è§£é‡Š
  âš¡ ä»£ç ä¼˜åŒ– - æ”¹è¿›ä»£ç æ€§èƒ½å’Œå¯è¯»æ€§
  ğŸ—£ï¸ å¯¹è¯äº¤æµ - è‡ªç„¶è¯­è¨€äº¤æµå„ç§ç¼–ç¨‹é—®é¢˜

ç¤ºä¾‹:
  "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è¯»å–CSVæ–‡ä»¶"
  "è§£é‡Šè¿™ä¸ªJavaScripté—­åŒ…çš„ç”¨æ³•"
  "ä¼˜åŒ–è¿™ä¸ªSQLæŸ¥è¯¢çš„æ€§èƒ½"

æ³¨æ„äº‹é¡¹:
  - æ¨¡å‹éœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æº
  - ç”Ÿæˆçš„ä»£ç å¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´
  - å»ºè®®ä½¿ç”¨è‹±æ–‡æè¿°è·å¾—æ›´å¥½æ•ˆæœ
    """
    print(help_text)

def main():
    parser = argparse.ArgumentParser(description="Qwen Coder - é€šä¹‰åƒé—®ä»£ç ç”Ÿæˆæ¨¡å‹")
    parser.add_argument("--mode", choices=["web", "streamlit", "cli"],
                       default="web", help="è¿è¡Œæ¨¡å¼ (é»˜è®¤: web)")
    parser.add_argument("--model", default="Qwen/Qwen2.5-Coder-7B-Instruct",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--port", type=int, default=7860,
                       help="Webç•Œé¢ç«¯å£ (é»˜è®¤: 7860)")

    args = parser.parse_args()

    print("ğŸš€ å¯åŠ¨ Qwen Coder...")
    print(f"æ¨¡å¼: {args.mode}")
    print(f"æ¨¡å‹: {args.model}")
    print("-" * 50)

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return

    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    try:
        import torch
        if torch.cuda.is_available():
            print(f"ğŸ® GPUå¯ç”¨: {torch.cuda.get_device_name()}")
        else:
            print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU (æ€§èƒ½è¾ƒæ…¢)")
    except ImportError:
        print("âš ï¸  PyTorchæœªå®‰è£…")

    # ä¸‹è½½æ¨¡å‹
    tokenizer, model = download_model(args.model)
    if tokenizer is None or model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œç£ç›˜ç©ºé—´")
        return

    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == "web":
        interface = create_web_interface(tokenizer, model)
        if interface:
            print(f"ğŸŒ å¯åŠ¨Webç•Œé¢: http://localhost:{args.port}")
            interface.launch(server_port=args.port, share=False)
        else:
            print("âŒ Webç•Œé¢åˆ›å»ºå¤±è´¥")

    elif args.mode == "streamlit":
        create_streamlit_app(tokenizer, model)

    elif args.mode == "cli":
        interactive_mode(tokenizer, model)

if __name__ == "__main__":
    main()
